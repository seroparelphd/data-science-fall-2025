{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra and Multivariable Calculus Self Assessment\n",
    "\n",
    "You can use numpy (or other software which supports linear algebra) to assist with any linear algebra computations.  \n",
    "\n",
    "If you are *extremely* sure that you can do these computations, you should just skim over the solutions and confirm that they make sense to you.  If you are confident that you could produce solutions which are of comparable or superior quality then the Linear Algebra and Multivariable Calculus parts of the Crash Course are probably not worth your time!\n",
    "\n",
    "1. Compute the total derivative $Df$ of the function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$ defined by $f(x,y) = (xy, x, x^2-y^2)$.  Use the linear approximation of $f$ at $(1,2)$ to approximate $f(1.01, 1.99)$.\n",
    "2. Consider the function $f: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $f(x,y,z) = x^2 + y^2 +z^2 - xy - 2x + 3y + z + 1$. Find the stationary points of $f$. Determine whether the Hessian is positive definite, positive semi-definite, negative semi-definite, negative definite, or none of the above.  What does this tell you about the stationary points:  are they local maxima, local minima, or saddle points?\n",
    "\n",
    "* Note:  The first two questions deal with multivariable differential calculus and optimization.  Many data science techniques involve specifying a model as a function of some parameters, and minimizing a loss function with respect to these parameters to \"fit\" the model.  An understanding of multivariate differential calculus is essential for really understanding what is happening with these techniques.  For instance: linear regression, logistic regression, support vector machines, and training a neural network all depend on minimizing a loss function of some parameters.\n",
    "\n",
    "3.  Project the vector $ \\vec{y} = \\begin{bmatrix}  1 \\\\ 2 \\\\ 3\\end{bmatrix}$ orthogonally onto the image of the linear transformation with matrix $A = \\begin{bmatrix} 1 & 2 \\\\ 1 & 3 \\\\ 1 & -1  \\end{bmatrix}$.  \n",
    "\n",
    "4. Find the Singular Value Decomposition of the matrix $\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6\\end{bmatrix}$.  Use the SVD to find a rank 1 approximation of the matrix.\n",
    "\n",
    "* Note:  Singular Value Decomposition is essential for understanding principle component analysis, which is an important dimension reduction technique often used in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Self Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Self Assessment\n",
    "\n",
    "> 1. Compute the total derivative $Df$ of the function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$ defined by $f(x,y) = (xy, x, x^2-y^2)$.  Use the linear approximation of $f$ at $(1,2)$ to approximate $f(1.01, 1.99)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Df\\big\\vert_{(x,y)} \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial}{\\partial x} (xy) & \\frac{\\partial}{\\partial y} (xy) \\\\\n",
    "\\frac{\\partial}{\\partial x} (x) & \\frac{\\partial}{\\partial y} (x) \\\\\n",
    "\\frac{\\partial}{\\partial x} (x^2 - y^2) & \\frac{\\partial}{\\partial y} (x^2 - y^2) \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "y & x \\\\\n",
    "1 & 0 \\\\\n",
    "2x & -2y \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "Df\\big\\vert_{(1,2)} \n",
    "= \n",
    "\\begin{bmatrix} \n",
    "2 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & -4 \\\\\n",
    "\\end{bmatrix}\\\\ \n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(1.01, 1.99) \n",
    "&= f( (1,2) + \\begin{bmatrix} 0.01 \\\\ -0.01\\end{bmatrix})\\\\\n",
    "&\\approx f(1,2) + Df\\big\\vert_{(1,2)}\\begin{bmatrix} 0.01 \\\\ -0.01\\end{bmatrix}\\\\\n",
    "&= (2,1,-3) + \\begin{bmatrix} \n",
    "2 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & -4 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix} 0.01 \\\\ -0.01\\end{bmatrix}\\\\\n",
    "&=(2,1,-3) + \\begin{bmatrix}0.01 \\\\ 0.01\\\\ -0.02 \\end{bmatrix}\\\\\n",
    "&= (2.01, 1.01, -2.94)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can check this against the exact value $f(1.01, 1.99) = (2.0099, 1.01, -2.94)$ which is very close!\n",
    "\n",
    "> 2. Consider the function $f: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $f(x,y,z) = x^2 + y^2 +z^2 - xy - 2x + 3y + z + 1$. Find the stationary points of $f$. Determine whether the Hessian is positive definite, positive semi-definite, negative semi-definite, negative definite, or none of the above.  What does this tell you about the stationary points:  are they local maxima, local minima, or saddle points?\n",
    "\n",
    "\n",
    "The gradient of $f$ is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f &= \\begin{bmatrix}  \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\\\ \\frac{\\partial f}{\\partial z}\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} 2x - y - 2 \\\\ 2y - x + 3  \\\\ 2z + 1\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the gradient is 0 when \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "2x - y = 2 \\\\ 2y - x   = -3 \\\\ 2z =  -1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We could solve this system by hand, but I will use numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stationary point is \n",
      " [[ 0.33333333]\n",
      " [-1.33333333]\n",
      " [-0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "M = np.array([[2, -1, 0 ],[-1, 2, 0],[0, 0, 2]])\n",
    "stationary_point = np.dot(linalg.inv(M), [[2],[-3],[-1]])\n",
    "print('The stationary point is \\n', stationary_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian matrix is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{H}f \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial z \\partial x}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2} & \\frac{\\partial^2 f}{\\partial z \\partial y}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial z} & \\frac{\\partial^2 f}{\\partial y \\partial z} & \\frac{\\partial^2 f}{\\partial z^2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix} \n",
    "2 & -1 & 0\\\\\n",
    "-1 & 2 & 0\\\\\n",
    "0 & 0 & 2\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We need to check the definiteness of this matrix.\n",
    "\n",
    "This is actually easy enough to do by inspection:  \n",
    "\n",
    "* $\\begin{bmatrix} 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ is an eigenvector with eigenvalue $2$\n",
    "* $\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$ is an eigenvector with eigenvalue $1$. \n",
    "\n",
    " Since the trace is $6$, we must have another eigenvector with eigenvalue $3$, so this matrix is positive definite.  Thus the critical point $(\\frac{1}{3}, -\\frac{4}{3}, -\\frac{1}{2})$ is where the global minimum value of this function occurs.  \n",
    "\n",
    "Addendum:  Oh, I see it now:  the eigenvector $\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$ has eigenvalue $3$.  Actually, we should have anticipated this, since by the real spectral theorem the eigenvectors of a symmetric matrix must be orthogonal.  Thinking geometrically, this vector must have been in the $xy$ plane to be orthogonal to the first eigenvector, and then to be perpendicular to $\\begin{bmatrix} 1 & 1 & 0 \\end{bmatrix}^\\top$ it had to be $\\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix}^\\top$!\n",
    "\n",
    "Let's double check ourselves with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues are  [3. 1. 2.]\n",
      "The eigenvectors are the columns of \n",
      " [[ 0.70710678  0.70710678  0.        ]\n",
      " [-0.70710678  0.70710678  0.        ]\n",
      " [ 0.          0.          1.        ]]\n",
      "These are the same eigenvectors we found, just normalized\n"
     ]
    }
   ],
   "source": [
    "H = np.array([[2,-1,0],[-1,2,0],[0,0,2]])\n",
    "\n",
    "# linalg.eig is a function which takes a matrix and returns a tuple of arrays. The index 0 element of the tuple is the array of eigevalues.\n",
    "# the index 1 tuple is the matrix whose columns are eigenvectors.\n",
    "\n",
    "print('The eigenvalues are ', linalg.eig(H)[0])\n",
    "print('The eigenvectors are the columns of \\n', linalg.eig(H)[1])\n",
    "print('These are the same eigenvectors we found, just normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3.  Project the vector $ \\vec{y} = \\begin{bmatrix}  1 \\\\ 2 \\\\ 3\\end{bmatrix}$ onto the image of the linear transformation with matrix $A = \\begin{bmatrix} 1 & 2 \\\\ 1 & 3 \\\\ 1 & -1  \\end{bmatrix}$\n",
    "\n",
    "There are several approaches.  In this case, when the dimension of the image is low, it is not unreasonable to use the Gramâ€“Schmidt process to find an orthonormal basis of the image, and then project $\\vec{y}$ onto the span of each basis vector.\n",
    "<br>\n",
    "I will take a different approach, which gives an explicit formula for the projection in terms of $A$ and $\\vec{y}$.  \n",
    "<br>\n",
    "Call the projected vector $\\vec{p}$.  Then we want $\\vec{p}$ to be in the image of $A$ so $\\vec{p} = A\\vec{x}$ for some $\\vec{x} \\in \\mathbb{R}^2$.  We also want $\\vec{y} - \\vec{p}$ to be perpedicular to the image of $A$.  So we want $A^\\top (\\vec{y} - \\vec{p}) = \\vec{0}$.\n",
    "<ve>\n",
    "Putting this together we have \n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "& A^\\top (\\vec{y} - A\\vec{x}) = \\vec{0}\\\\\n",
    "& A^\\top A\\vec{x} = A^\\top \\vec{y}\\\\\n",
    "& \\vec{x} = (A^\\top A)^{-1} (A^\\top \\vec{y}), \\textrm{see Note}\\\\\n",
    "& A \\vec{x} = A (A^\\top A)^{-1} (A^\\top \\vec{y})\\\\\n",
    "& p = A (A^\\top A)^{-1} (A^\\top \\vec{y})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note 1:  $(A^\\top A)$ is invertible when $A$ has linearly independent columns, which holds in our example.\n",
    "\n",
    "Note 2:  This is the \"normal equation\" for least squares linear regression!  We will see that connection spelled out in a later notebook.\n",
    "\n",
    "We could do this by hand since $A$ is just a $3 \\times 2$ matrix, but I will use numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.76923077]\n",
      " [1.42307692]\n",
      " [2.80769231]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[1],[2],[3]])\n",
    "A = np.array([[1,2],[1,3],[1,-1]])\n",
    "Aty = np.dot(A.transpose(),y) # A-tranpose times y\n",
    "AtAinv = linalg.inv(np.dot(A.transpose(), A)) # the inverse of A-tranpose times A.\n",
    "p = np.dot(A, np.dot(AtAinv, Aty)) # writing out the formula we found.\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, -0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check that y - p is perpendicular to both columns of A.  It is clearly in the image of A since it is defined as A applied to a vector.\n",
    "# Rounded to 10 place values, these dot products are both 0.\n",
    "\n",
    "round(np.dot(A[:,0].reshape(1,3),(y-p))[0,0],10), round(np.dot(A[:,1].reshape(1,3),(y-p))[0,0],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Find the singular value decomposition of the matrix $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6\\end{bmatrix}$.  Use the SVD to find a rank 1 approximation of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4],[5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVDResult(U=array([[-0.2298477 ,  0.88346102,  0.40824829],\n",
       "       [-0.52474482,  0.24078249, -0.81649658],\n",
       "       [-0.81964194, -0.40189603,  0.40824829]]), S=array([9.52551809, 0.51430058]), Vh=array([[-0.61962948, -0.78489445],\n",
       "       [-0.78489445,  0.61962948]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      " [[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "S = \n",
      " [[9.52551809 0.        ]\n",
      " [0.         0.51430058]\n",
      " [0.         0.        ]]\n",
      "V = \n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n",
      "USV^t = \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# U is self-explanatory\n",
    "U = linalg.svd(A)[0]\n",
    "\n",
    "# Looking at the documentation, S is the vector of singular values. \n",
    "# We actually want the matrix with appropriate dimension.  Using np.diag(linalg.svd(A)[1]) gives us a 2x2 matrix. \n",
    "# To make it 2 x 3 we need to pad that with a row of zeros at the bottom.\n",
    "\n",
    "S = np.pad(np.diag(v=linalg.svd(A)[1]),pad_width=[(0,1),(0,0)], mode='constant', constant_values=0)\n",
    "\n",
    "# The method returns V tranpose, not V.\n",
    "\n",
    "V = linalg.svd(A)[2].transpose()\n",
    "\n",
    "# Check that it works:\n",
    "\n",
    "print(\"U = \\n\", U)\n",
    "print(\"S = \\n\", S)\n",
    "print(\"V = \\n\", V)\n",
    "print(\"USV^t = \\n\", U @ S @ V.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank 1 approximation of $A$ is just $U \\Sigma V^\\top$, but replacing the smaller of the two eigenvalues with $0$.  So our rank $1$ approximation of $A$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35662819 1.71846235]\n",
      " [3.09719707 3.92326845]\n",
      " [4.83776596 6.12807454]]\n"
     ]
    }
   ],
   "source": [
    "rank_1_approx = U  @ (S * [1,0]) @ V.transpose()\n",
    "print(rank_1_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that rank_1_approx is close to $A$, but its second column is a multiple of its first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These values should all be the same: [1.26671579 1.26671579 1.26671579]\n"
     ]
    }
   ],
   "source": [
    "print('These values should all be the same:', rank_1_approx[:,1]/rank_1_approx[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
