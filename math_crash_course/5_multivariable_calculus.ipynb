{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash Course Lesson 5:\n",
    "\n",
    "In this lesson you will learn about:\n",
    "\n",
    "*  The **total derivative** of a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "*  The **gradient** of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n",
    "    * How to do **gradient descent** to find local minimum values of a function.\n",
    "*  The **Hessian** of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n",
    "    * How to use the eigenvalues of the Hessian to characterize critical points as local max, local min, or saddle point.\n",
    "\n",
    "I assume that you you already know about partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**:  Let $f: \\mathbb{R}^n \\to \\mathbb{R}^m$.  Let $p \\in \\mathbb{R}^n$.  Then $f$ is called **differentiable** at $p$ if there is a linear map $Df\\big\\vert_p : \\mathbb{R}^n \\to \\mathbb{R}^m$ so that we have the approximation\n",
    "\n",
    "$$\n",
    "f(p+\\vec{h}) = f(p)+ Df\\big\\vert_p(\\vec{h}) + \\textrm{Error}(\\vec{h})\n",
    "$$\n",
    "\n",
    "where the error function satisfies the following constraint:\n",
    "\n",
    "$$\n",
    "\\lim_{\\vec{h} \\to \\vec{0}} \\frac{\\vert \\textrm{Error}(\\vec{h}) \\vert}{\\vert \\vec{h} \\vert} = 0\n",
    "$$\n",
    "\n",
    "**Theorem**:  The matrix of $Df\\big\\vert_p$ with respect to the standard basis is the **Jacobian matrix** of partial derivatives of $f$ at $p$:\n",
    "\n",
    "$$\n",
    "Df\\big\\vert_p = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_1}{\\partial x_3} & \\dots & \\frac{\\partial f_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_3} & \\dots & \\frac{\\partial f_2}{\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\frac{\\partial f_m}{\\partial x_3} & \\dots & \\frac{\\partial f_m}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here we are using $(x_1, x_2, x_3, \\dots, x_n)$ as the coordinates of the domain $\\mathbb{R}^n$, and we are using $f_j(p)$ to refer to the component of $f(p)$ in the $j^{th}$ dimension of the codomain $\\mathbb{R}^m$:  $$f(p) = (f_1(p), f_2(p), f_3(p), \\dots, f_m(p))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That definition is a lot to take in!  I will explain it with the aid of this beautiful picture (which I slightly modified) from the Creative Commons Licensed [Calculus (OpenStax) section 15.7](https://math.libretexts.org/Bookshelves/Calculus/Calculus_%28OpenStax%29/15%3A_Multiple_Integration/15.07%3A_Change_of_Variables_in_Multiple_Integrals):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = 'middle'>\n",
    "<img src=\"crash_course_assets/CNX_Calc_Figure_15_07_005.jpg\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a picture of a function $f : \\mathbb{R}^2 \\to \\mathbb{R}^2$.  We are using $(u,v)$ as the coordinates for the domain and $(x,y)$ as the coordinates for the codomain.\n",
    "\n",
    "We have a point $p$, and a small rectangle attached to $p$ representing possible values of $\\vec{h}$.  Each $\\vec{h}$ will be a linear combination $\\Delta u \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} + \\Delta v \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (we are using $\\Delta u$ and $\\Delta v$ as the names for the coefficients in this situation to remind you that they are supposed to be \"small\" changes for the approximation to be good).\n",
    "\n",
    "You can see that applying $f$ to the orange rectangle gives you a slightly \"curved parallelogram\" in the codomain.  Since $f$ is not linear, we should expect that straight lines will not stay straight:  they will be curved.\n",
    "\n",
    "However, when the orange rectangle is small enough, the blue rectangle will become closer and closer to a parallelogram.  The two vectors defining the sides of this parallelogram would be $Df\\big\\vert_p(\\Delta u \\vec{e_u})$ and $Df\\big\\vert_p(\\Delta u \\vec{e_v})$.  \n",
    "\n",
    "So when we \"zoom in\" on a small region around $f$, the function should be **approximately linear**.  This linear approximation is $Df\\big\\vert_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a concrete example.\n",
    "\n",
    "Let $f(x,y) = (x+xy, x^2y, x+2y)$.\n",
    "\n",
    "Then we have the Jacobian matrix\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x}  & \\frac{\\partial f_1}{\\partial y}\\\\\n",
    "    \\frac{\\partial f_2}{\\partial x}  & \\frac{\\partial f_3}{\\partial y}\\\\\n",
    "    \\frac{\\partial f_3}{\\partial x}  & \\frac{\\partial f_3}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial} {\\partial x} (x+xy) & \\frac{\\partial}{\\partial y} (x+xy)\\\\\n",
    "    \\frac{\\partial}{\\partial x} (x^2y)  & \\frac{\\partial}{\\partial y} (x^2y)\\\\\n",
    "    \\frac{\\partial }{\\partial x} (x+2y)  & \\frac{\\partial}{\\partial y} (x+2y)\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    1+y & x\\\\\n",
    "    2xy  & x^2\\\\\n",
    "    1 & 2\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We are supposed to have the approximation for small values of $\\Delta x$ and $\\Delta y$:\n",
    "\n",
    "$$\n",
    "f\\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} \\Delta x \\\\ \\Delta y \\end{bmatrix}\\right) \\approx f\\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\right) + \\begin{bmatrix}\n",
    "    1+y & x\\\\\n",
    "    2xy  & x^2\\\\\n",
    "    1 & 2\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix} \\Delta x \\\\ \\Delta y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's test it with $x = 3$, $y = 4$, $\\Delta x = 0.001$, and $\\Delta y = 0.002$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f\\left( \\begin{bmatrix} 3.001 \\\\ 4.002 \\end{bmatrix}\\right) &\\stackrel{?}{\\approx} f\\left( \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\right) + \n",
    "\\begin{bmatrix}\n",
    "    5 & 3 \\\\\n",
    "    24  & 9\\\\\n",
    "    1 & 2\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix} 0.001 \\\\ 0.002\\end{bmatrix}\\\\\n",
    "\\left(15.011002, 36.042016, 11.005  \\right) &\\stackrel{?}{\\approx} \\left( 15, 36, 11 \\right) + \\begin{bmatrix}0.011 \\\\0.042 \\\\ 0.005 \\end{bmatrix}\\\\\n",
    "\\left(15.011002, 36.042016, 11.005  \\right) &\\stackrel{?}{\\approx} \\left( 15.011, 36.042, 11.005 \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The linear approximation is doing a pretty good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**  Write a python function $\\textrm{Jacobian}(f, p, h)$ meeting the following specifications:\n",
    "\n",
    "* $f$ is a python function which takes a NumPy array as input and returns a NumPy array as output.  For example, we would implement the example function above as \n",
    "\n",
    "\n",
    "```python\n",
    "def f(p):\n",
    "    x = p[0]\n",
    "    y = p[1]\n",
    "    return np.array([x+x*y, x**2*y, x+2*y])\n",
    "``` \n",
    "\n",
    "* $p$ is a numpy array.  In the example above, we would have \n",
    "\n",
    "```python \n",
    "p = np.array([3,4]) \n",
    "```\n",
    "\n",
    "* $h$ is a float.\n",
    "\n",
    "```python\n",
    "h = 0.001\n",
    "```\n",
    "\n",
    "* $\\textrm{Jacobian}(f, p, h)$ should return a numerical approximation of the Jacobian matrix using the [symmetric difference quotient](https://en.wikipedia.org/wiki/Symmetric_derivative) to approximate the partial derivatives.  In other words, you will approximate\n",
    "\n",
    "$$\n",
    "\\left.\\frac{\\partial f_i}{\\partial x_j}\\right\\vert_p \\approx \\frac{f_i(p_j+h) - f_i(p_j- h)}{2h}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def Jacobian(f,p,h):\n",
    "    # your code here\n",
    "    # return J\n",
    "\n",
    "# test\n",
    "\n",
    "# def f(p):\n",
    "#     x = p[0]\n",
    "#     y = p[1]\n",
    "#     return np.array([x+x*y, x**2*y, x+2*y])\n",
    "\n",
    "# p = np.array([3,4])\n",
    "\n",
    "# h = 0.001\n",
    "\n",
    "# print(Jacobian(f,p,h))\n",
    "\n",
    "# should output\n",
    "\n",
    "#[[ 5.  3.]\n",
    "# [24.  9.]\n",
    "# [ 1.  2.]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions $f:\\mathbb{R}^n \\to \\mathbb{R}$ warrant special attention.\n",
    "\n",
    "In that case, the Jacobian matrix would just be a single row:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} &  \\frac{\\partial f}{\\partial x_2} & \\frac{\\partial f}{\\partial x_3} & \\dots & \\frac{\\partial f}{\\partial x_n}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The transpose of this row would be a vector.  We call that vector the **gradient** vector, and use the special notation $\\nabla f$ for it.\n",
    "\n",
    "In this case, we can rewrite our linear approximation using a dot product instead of a matrix multiplication, since a row matrix times a column matrix is the same as taking the dot product of the row with the column.\n",
    "\n",
    "$$\n",
    "f(p + \\vec{h}) \\approx f(p) + \\nabla f\\big\\vert_p \\cdot \\vec{h}\n",
    "$$\n",
    "\n",
    "We can now translate some things we know about dot products into statements about the gradient.\n",
    "\n",
    "* If we think about fixing $p$ and letting $\\vec{h}$ change with fixed length, $\\nabla f\\big\\vert_p \\cdot \\vec{h}$ will most maximized when $\\vec{h}$ is a multiply of $\\nabla f\\big\\vert_p$.  The reason is that, geometrically, $\\nabla f\\big\\vert_p \\cdot \\vec{h}$ is proportional to the signed length of the orthogonal projection of $\\vec{h}$ onto $\\nabla f\\big\\vert_p$.  This projection is maximized when $\\vec{h}$ points in the same direction as $\\nabla f\\big\\vert_p$.\n",
    "    * This is often paraphrased as \"$\\nabla f\\big\\vert_p$ points in the direction of greatest increase\".\n",
    "* Similarly $-\\nabla f\\big\\vert_p$ points in the direction of greatest decrease!\n",
    "* If $\\nabla f\\big\\vert_p \\cdot \\vec{h} = 0$, then $f$ is not changing much as you change the input from $p$ to $p+\\vec{h}$.  So $\\vec{h}$ is a tangent vector to a **level surface** of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the gradient to find local minima of functions using a technique known as the **gradient descent algorithm**:\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$\n",
    "\n",
    "* Choose a starting point $p \\in \\mathbb{R}^n$.\n",
    "* Choose a \"learning rate\" $lr > 0$.\n",
    "* Update $p_{\\textrm{new}} = p_{\\textrm{old}} - lr \\nabla f \\big\\vert_p$\n",
    "* Repeat until you reach some termination criteria.\n",
    "* Common terminantion criteria include:\n",
    "    * $|f(p_{\\textrm{new}}) - f(p_{\\textrm{old}})| < \\textrm{tolerance}$ for some prechosen tolerance.\n",
    "    * $|\\nabla f \\big\\vert_p| < \\textrm{tolerance}$\n",
    "    * Some maximum number of iterations reached\n",
    "\n",
    "Notes:  \n",
    "\n",
    "* This algorithm doesn't always converge.\n",
    "    * You can try adjusting the learning rate or initial choice of $p$ if you don't get convergence.\n",
    "    * You can also adjust the learning rate dynamically instead of having it a fixed constant.\n",
    "* When it does converge, it might only give a local minimum, not the global minimum.\n",
    "* **This is at the heart of a lot of machine learning**:  we will specify a model of a desired form, but which depends on some parameters.  We compare the outputs of our model to the outputs of the training data using some \"loss function\".  The way we \"train\" a machine learning model is by adjusting the parameters using some type of gradient descent algorithm (there are a lot of tweaks to the basic set up) to make the loss smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**:  Reuse your Jacobian Code to write a python function\n",
    "\n",
    "```python\n",
    "# f is the same type of function we used in Jacobian.\n",
    "# p is our initial guess\n",
    "# h is the parameter we use for our partial derivative approximation.  Jacobian(f,p,h) should give you a row vector of partials.\n",
    "# lr is the learning rate\n",
    "# Have the termination criteria for the loop be np.linalg.norm( gradient) < tolerance.\n",
    "\n",
    "def grad_desc(f, p, h, lr, tolerance):\n",
    "    # your code here\n",
    "    return p\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code grad_desc here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multivariable version of the second derivative is the **Hessian** matrix:\n",
    "\n",
    "**Definition:**  The **Hessian** of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ at a point $p$ is the symmetric matrix:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}f\\big\\vert_p = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_3} & \\dots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2\\partial x_2} & \\frac{\\partial^2 f}{\\partial x_2\\partial x_3} & \\dots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_n}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_3\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_3\\partial x_2} & \\frac{\\partial^2 f}{\\partial x_3\\partial x_3} & \\dots & \\frac{\\partial^2 f}{\\partial x_3\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n\\partial x_2} & \\frac{\\partial^2 f}{\\partial x_n\\partial x_3} & \\dots & \\frac{\\partial^2 f}{\\partial x_n\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Theorem:** The least precise statment of the second order Talyor approximation of all time:\n",
    "\n",
    "$$f(p + \\vec{h}) \\approx f(p) + \\nabla f\\big\\vert_p \\cdot \\vec{h} + \\frac{1}{2} \\vec{h}^\\top \\mathcal{H}f\\big\\vert_p \\vec{h} $$\n",
    "\n",
    "This gives the best approximation of $f$ near $p$ as a quadratic polynomial of $n$ variables.\n",
    "\n",
    "**Definition**:  A symmetric matrix $A$ is called:\n",
    "* **positive definite** if $\\vec{v}^\\top A \\vec{v} > 0$ for all nonzero vectors $\\vec{v}$\n",
    "* **postivie semidefinite** if $\\vec{v}^\\top A \\vec{v} \\geq 0$ for all nonzero vectors $\\vec{v}$\n",
    "* **negative definite** if $\\vec{v}^\\top A \\vec{v} < 0$ for all nonzero vectors $\\vec{v}$\n",
    "* **negative semidefinite** if $\\vec{v}^\\top A \\vec{v} \\leq 0$ for all nonzero vectors $\\vec{v}$\n",
    "* **indefinite** if the sign of $\\vec{v}^\\top A \\vec{v}$ is sometimes positive and sometimes negative\n",
    "\n",
    "**Theorem:** The definiteness of a symmetric matrix $A$ can be determined from its eigenvalues:\n",
    "* **positive definite** if all eigenvalues are positive\n",
    "* **postivie semidefinite** if all eigenvalues are non-negative\n",
    "* **negative definite** if all eigenvalues are negative\n",
    "* **negative semidefinite** is all eigenvalues are non-positive\n",
    "* **indefinite** if it has some positive and some negative eigenvalues.\n",
    "\n",
    "**Theorem**: (Multivariable second derivative test)  Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ be a function, and $p$ be a stationary point of $f$ (which means that $\\nabla f \\big\\vert_p = \\vec{0}$).  Then $p$ is\n",
    "\n",
    "* A local minimum of $f$ if the Hessian of $f$ is positive definite.\n",
    "* A local maximmum of $f$ if the Hessian is negative definite.\n",
    "* A saddle point of $f$ if the Hessian is independite.\n",
    "* We don't know whether $f$ is a local max or min if the Hessian is only positive semidefinite or negative semidefinite.  It could go \"either way\" in the direction of the eigenvectors with eigenvalue $0$ \n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "* Many classical statistical techniques fall under the general umbrella of [convex optimization problems](https://en.wikipedia.org/wiki/Convex_optimization), where we are minimizing a function whose Hessian is positive definite.  For instance, fitting logistic regression is such a problem.  We can often do a **lot** better than vanilla gradient descent when solving such problems.  One method, which uses the \"curvature\" information contained in the Hessian is [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization).\n",
    "* Most deep learning optimizers do not use Hessians because they are expensive to compute (especially when you have a lot of parameters), but this is not universally true.\n",
    "* Even if Hessians are not directly relevant to your problem, it is conceptually useful to understand the classification of critical points when thinking about loss functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3.]\n",
      " [24.  9.]\n",
      " [ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Jacobian(f, p, h):\n",
    "    n = p.shape[0] # The dimension of the domain.\n",
    "    m = f(p).shape[0] # The dimension of the codomain.\n",
    "    J = np.zeros((m,n)) # Initializing J as a matrix zeros of the appropriate shape.\n",
    "    for i in range(m): # looping through the output dimension.\n",
    "        for j in range(n): # looping through the input dimension.\n",
    "            e_j = np.zeros((n)) # initialize the jth basis vector as a vector of zeros.\n",
    "            e_j[j] = 1 # set the jth component to 1\n",
    "            partial = (f(p+h*e_j) - f(p-h*e_j))[i]/(2*h) # computing the approximation of the partial derivative.\n",
    "            J[i,j] = partial # setting the [i,j] entry of J to this partial\n",
    "    return J\n",
    "\n",
    "# test\n",
    "\n",
    "def f(p):\n",
    "    x = p[0]\n",
    "    y = p[1]\n",
    "    return np.array([x+x*y, x**2*y, x+2*y])\n",
    "\n",
    "p = np.array([3,4])\n",
    "\n",
    "h = 0.001\n",
    "\n",
    "print(Jacobian(f,p,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66435975  0.33564025]\n"
     ]
    }
   ],
   "source": [
    "def grad_desc(f, p, h, lr, tolerance):\n",
    "    grad = Jacobian(f, p, h) \n",
    "    while np.linalg.norm(grad) > tolerance:\n",
    "        p = p-lr*Jacobian(f, p, h).reshape(p.size)\n",
    "        grad = Jacobian(f, p, h)\n",
    "    return p\n",
    "\n",
    "# test\n",
    "\n",
    "\n",
    "\n",
    "p = np.array([3,4])\n",
    "\n",
    "def f(p):\n",
    "    x = p[0]\n",
    "    y = p[1]\n",
    "    return np.array([x**2 +x*y + y**2 + x])\n",
    "\n",
    "h = 0.001\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "tolerance = 0.01\n",
    "\n",
    "print(grad_desc(f, p, h, lr, tolerance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
