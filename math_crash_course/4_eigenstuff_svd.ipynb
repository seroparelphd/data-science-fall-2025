{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash Course Lesson 4\n",
    "\n",
    "In this lesson we will learn about:\n",
    "\n",
    "* **Change of Basis**\n",
    "* **Eigenvectors** and **Eigenvalues**\n",
    "* **Singular Value Decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we defined the matrix $M_L$ associated with a linear map $L: \\mathbb{R}^n \\to \\mathbb{R}^m$ we said that the columns of $M_L$ should be the images of each of the **standard basis vectors** $\\vec{e}_1, \\vec{e_2}, \\vec{e}_3, ..., \\vec{e}_n$.  We were also implicitly using the standard basis of $\\mathbb{R}^m$:  when we write a vector like $\\begin{bmatrix} 5 \\\\ 3 \\end{bmatrix}$ we really mean $5\\begin{bmatrix}1 \\\\ 0 \\end{bmatrix} + 3 \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$.\n",
    "\n",
    "However **any** basis is capable of representing any arbitrary vector as a linear combination of the basis vectors, in a unique way.  The coefficients of this linear combination can be thought of as a new set of coordinates for the vector.  \n",
    "\n",
    "**Example**:  Let $\\mathcal{B} = \\left( \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right) = (\\vec{b}_1, \\vec{b}_2)$.  This is a basis of $\\mathbb{R}^2$.  Every vector in $\\mathbb{R}^2$ can be expressed as a linear combination of these vectors.  We can even make a coordinate system to see how:\n",
    "\n",
    "<p align = 'middle'>\n",
    "<img src=\"crash_course_assets/standard-basis.png\" width=\"400\">\n",
    "<img src=\"crash_course_assets/new-basis.png\" width=\"400\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the point \n",
    "\n",
    "$$\\begin{bmatrix} 5 \\\\ 3  \\end{bmatrix} = 5\\begin{bmatrix}1 \\\\ 0 \\end{bmatrix} + 3 \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = 5 \\vec{e}_1 + 3 \\vec{e}_2$$\n",
    "\n",
    "could also be thought of as \n",
    "\n",
    "$$\n",
    "2\\vec{b}_1 + 3 \\vec{b}_2 = 2\\begin{bmatrix}1 \\\\ 0 \\end{bmatrix} + 3 \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 3  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So the same vector has coordinates $\\begin{bmatrix} 5 \\\\ 3  \\end{bmatrix}$ with respect to the standard basis, but has coordinates $\\begin{bmatrix} 2 \\\\ 3  \\end{bmatrix}_\\mathcal{B}$ with respect to the basis $\\mathcal{B}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**  Let $\\mathcal{B} = (\\vec{b_1}, \\vec{b}_2, \\vec{b_3}, \\dots , \\vec{b}_n)$ be a basis of $\\mathbb{R}^n$ and $\\vec{v} \\in \\mathbb{R}^n$.  Then we can write $\\vec{v}$ uniquely as a linear combination of the vectors from $\\mathcal{B}$:\n",
    "\n",
    "$$\n",
    "\\vec{v} = c_1\\vec{b_1} +c_2 \\vec{b}_2 + c_3\\vec{b}_3 + \\dots + c_n \\vec{b}_n\n",
    "$$\n",
    "\n",
    "We call these coefficients $c_i$ the **coordinates of $\\vec{v}$ with respect to $\\mathcal{B}$**.  We introduce the following notation as a shorthand for the linear combination:\n",
    "\n",
    "$$\n",
    "c_1\\vec{b_1} +c_2 \\vec{b}_2 + c_3\\vec{b}_3 + \\dots + c_n \\vec{b}_n = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\ c_n \\end{bmatrix}_\\mathcal{B}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:  Using our understanding of the equivalence between linear combination of column vectors with matrix multiplication, we could also rephrase this as follows:\n",
    "\n",
    "Let $M_\\mathcal{B}$ be the matrix whose columns are $\\mathcal{B}$ (in standard coordinates).  Then the coordinates of $\\vec{v} \\in \\mathbb{R}^n$ with respect to $\\mathcal{B}$ is the vector of coefficients $\\vec{c}$ which solves the equation \n",
    "\n",
    "$$\n",
    "\\vec{v} = M_\\mathcal{B} \\vec{c}\n",
    "$$\n",
    "\n",
    "This can be solved using the inverse of $M_\\mathcal{B}$.\n",
    "\n",
    "$$\n",
    "\\vec{c} = M_\\mathcal{B}^{-1} \\vec{v}\n",
    "$$\n",
    "\n",
    "**Moral**:  To find the coordinates of a vector $\\vec{v}$ with respect to a new basis $\\mathcal{B}$, just apply the inverse of the matrix whose columns come from $\\mathcal{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coordinates of \n",
      " [[5]\n",
      " [3]] \n",
      " with respect to the columns of \n",
      " [[1 1]\n",
      " [0 1]] \n",
      " are \n",
      " [[2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "# Showing that this works using the example we kicked things off with.\n",
    "\n",
    "import numpy as np\n",
    "v = np.array([[5],[3]])\n",
    "M = np.array([[1,1],[0,1]])\n",
    "Minv = np.linalg.inv(M)\n",
    "v_new = np.dot(Minv, v)\n",
    "print('The coordinates of \\n', v, '\\n with respect to the columns of \\n', M, '\\n are \\n', v_new )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**:  Find the coordinates of $\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix}$ with respect to the basis \n",
    "\n",
    "$$\\mathcal{B} = \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\begin{bmatrix} 1 \\\\ -1 \\\\ 0\\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ -2\\end{bmatrix} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience has shown that when confronting a problem where we think linear algebra might help, it is extremely helpful to find bases for the spaces we are interested in which are \"custom made\" to make our problem easy to work with.\n",
    "\n",
    "For example, we have had a special focus on orthogonal projection onto a $k$ dimensional subspace of $\\mathbb{R}^n$ throughout this crash course.  We can view the Gram-Schmidt process as a way to \"custom make\" a basis for $\\mathbb{R}^n$ where all the basis vectors are orthogonal and the first $k$ basis vectors span the subspace.  We have seen how this is a useful basis for the task of orthogonally projecting onto the subspace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**:  We now define the **matrix of a linear map with respect to a choice of basis for the domain and codomain**.  Let $L : \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear map.  Let $\\mathcal{B}_1$ be a basis for $\\mathbb{R}^n$ and $\\mathcal{B}_2$ be a basis for $\\mathbb{R}^m$.  Then we define the matrix for $L$ with respect to these bases to be the matrix $M_{\\mathcal{B}_1 \\to \\mathcal{B}_2}$ whose columns are the $\\mathcal{B}_2$ coordinates of the image (under $L$) of the basis vectors from $\\mathcal{B}_1$.\n",
    "\n",
    "**Example**:  Let $L: \\mathbb{R}^2 \\to \\mathbb{R}^3$ have standard matrix\n",
    "\n",
    "$$\n",
    "M_L = \\begin{bmatrix} 1 & 2 \\\\ 0 & -1 \\\\ -1 & 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $\\mathcal{B}_1$ be the columns of the matrix $B_1$:\n",
    "\n",
    "$$\n",
    "B_1 = \\begin{bmatrix} 2  & -1\\\\ -1 & 1  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $\\mathcal{B}_2$ be the columns of the matrix $B_2$:\n",
    "\n",
    "$$\n",
    "B_2 = \\begin{bmatrix} 1 & -1 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we can compute the matrix $M_{\\mathcal{B}_1 \\to \\mathcal{B}_2}$ as follows:\n",
    "\n",
    "Note that the first column is $B_2^{-1}M_L(B_1 \\vec{e}_1)$ and the second column is $B_2^{-1}M_L(B_1 \\vec{e}_2)$.  But that means that the matrix $M_{\\mathcal{B}_1 \\to \\mathcal{B}_2}$ is given by\n",
    "\n",
    "$$\n",
    "M_{\\mathcal{B}_1 \\to \\mathcal{B}_2} = B_2^{-1} M_L B_1\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5  1.5]\n",
      " [-2.5  0.5]\n",
      " [ 0.  -3. ]]\n"
     ]
    }
   ],
   "source": [
    "B_1 = np.array([[2,-1],[1,1]])\n",
    "B_2 = np.array([[1,-1,0],[1, 1, 1],[1, 1, 0]])\n",
    "B_2_inv = np.linalg.inv(B_2)\n",
    "M = np.array([[1,2],[0,-1],[-1,1]])\n",
    "M_new = np.dot(B_2_inv, np.dot(M, B_1))\n",
    "print(M_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors\n",
    "\n",
    "The easiest linear transformations to understand are the *scaling transformations*  $L(\\vec{v}) = cL(\\vec{v]})$ for some constant $c$.  Visually, they just stretch all vectors by the same constant factor, either enlarging them or reducing them in size.\n",
    "\n",
    "The next easiest linear transformations to understand scale each coordinate axis independently.  For example the map\n",
    "\n",
    "$$\n",
    "L\\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\right) = \\begin{bmatrix} 2x \\\\ 3y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "stretches the plane horizontally by a factor of 2 and vertically by a factor of 3.\n",
    "\n",
    "The matrix for such a linear transformation is diagonal:\n",
    "\n",
    "$$\n",
    "M_L = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we have a linear transformation from $\\mathbb{R}^2 \\to \\mathbb{R}^2$ which *doesn't* have a diagonal matrix, it can be pretty hard to have a geometric understanding of what it is doing.  You know it is rotating, stretching, and shearing space somehow, but it is certainly not as simple as the diagonal matrix transformations we talked about above.\n",
    "\n",
    "Often we are able to find a basis of **eigenvectors** for linear transformations.  The matrix with respect to a basis of eigenvectors is diagonal, so it makes understanding the linear transformation much simpler!\n",
    "\n",
    "**Definition**:  Let $L:\\mathbb{R}^n \\to \\mathbb{R}^n$ be a linear transformation.  A vector $\\vec{v}$ is called an **eigenvector** of $L$ if $\\vec{v} \\neq  \\vec{0}$ and there is a scalar $\\lambda \\in \\mathbb{R}$ so that \n",
    "\n",
    "$$\n",
    "L(\\vec{v}) = \\lambda \\vec{v}\n",
    "$$\n",
    "\n",
    "This constant $\\lambda$ is called the **eigenvalue** associated with $\\vec{v}$.\n",
    "\n",
    "The effect of $L$ on $\\textrm{span}(\\vec{v})$ is just \"simple scaling\"!\n",
    "\n",
    "Note:  While it is not standard terminology I will use the phrase **eigenstuff** of $L$ to refer to the eigenvector/eigenvalue pairs of $L$.  I am doing this because I hope it will catch on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**  Let $L: \\mathbb{R}^2 \\to \\mathbb{R}^2$ have matrix\n",
    "\n",
    "$$\n",
    "M_L = \\begin{bmatrix} -1 & 3 \\\\ 1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ is an eigenvector with eigenvalue $2$ and $\\begin{bmatrix} -3 \\\\ 1 \\end{bmatrix}$ is an eigenvector with eigenvalue $-2$ (check!)\n",
    "\n",
    "These vectors are linearly independent so they form a basis of eigevectors which I will call $\\mathcal{B}$.\n",
    "\n",
    "Let's see how this gives us a better geometric understanding of $L$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture on below shows the standard basis vectors $\\vec{e}_1$ and $\\vec{e_2}$ as solid red and blue vectors.  The image of these vectors under $L$ are dotted red and blue vectors (these are the columns of $M_L$).  It is pretty hard to understand exactly what $L$ is doing to the plane.  It seems like it is flipping it, scaling it, and shearing it somehow, but it is a little baffling.  The linear transformation is \"mixing up\" the coordinates somehow.\n",
    "\n",
    "<p align = 'middle'>\n",
    "<img src=\"crash_course_assets/bad-basis.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Now let us look at exactly the same linear transformation, but using the basis of eigenvectors $\\mathcal{B}$.  Again the first basis vector is a solid red vector and the second basis vector is a solid blue vector.  The dotted vectors represent where they are mapped by $L$.  This is so much easier to understand!  From this picture, we can see that $L$ just stretches the plane by a factor of $2$ in the direction of the first basis vector, and stretches the plane by a factor of $-2$ (which flips it and stretches it) in the direction of the second basis vector. \n",
    "\n",
    "<p align = 'middle'>\n",
    "<img src=\"crash_course_assets/good-basis.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "The matrix of $L$ with respect to $\\mathcal{B}$ is as simple as possible:\n",
    "\n",
    "$$\n",
    "M_{\\mathcal{B}} = \\begin{bmatrix}  2 & 0 \\\\ 0 & -2\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few facts about eigenvectors:\n",
    "\n",
    "* If you took an undergraduate linear algebra course you were probably taught to find eigenstuff using determinants and something called the \"characteristic equation\".  I would like to note that this is **completely impractical** for anything except the smallest matrices.\n",
    "* It is usually impossible to find \"closed form\" expressions for eigenstuff:  you will usually not be able to get exact expressions for them.  The best we can do is numerically approximate.\n",
    "* See [this wikipedia page](https://en.wikipedia.org/wiki/Eigenvalue_algorithm) to get an idea of the variety of algorithms available for finding eigenstuff.  You don't need to necessarily need to learn any of these, but you should be aware that if your matrix is special in some way (symmetric, tridiagonal, \"sparse\", etc) you may get better performance out of an algorithm which is tailored to that particular case.\n",
    "* Not all linear transformations have real eigenstuff.  For instance, the matrix\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}  0 & -1 \\\\ 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "has the geometric effect of rotation by $90$ degrees (check!).  This doesn't have any \"real\" eigenstuff.  However, if you allow complex numbers (something we will not consider in this crash course!) every linear transformation does have at least one eigenvector.\n",
    "* Even allowing complex numbers, not every matrix has a **basis** of eigenvectors, as demonstrated by the counterexample:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 0 & 1 \\\\\n",
    " 0 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}$ is clearly an eigenvector for this matrix with eigenvalue $0$, but there can be no other eigenstuff:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 0 & 1 \\\\\n",
    " 0 & 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \n",
    " \\begin{bmatrix}\n",
    "  y \\\\ 0 \n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "for $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$ to be an eigenvector we would need $\\lambda x = y$ and $\\lambda y = 0$.  If $y = 0$ this would imply either $x = 0$ (not allowed, since eigenvectors cannot be the zero vector) or $\\lambda = 0$ (which would imply $y = 0$, and hence the eigenvector is of the form $\\begin{bmatrix}x \\\\ 0 \\end{bmatrix} =  x \\begin{bmatrix}1 \\\\ 0 \\end{bmatrix}$, which is the eigenvector we already found).  If you want to know more about this situation you should check out the wikipiedia page for [Jordan Canonical Form](https://en.wikipedia.org/wiki/Jordan_normal_form)\n",
    "\n",
    "* However, we can say that \"with probability 1\" a random linear map $: \\mathbb{C}^n \\to \\mathbb{C}^n$ will have a basis of eigenvectors, so \" for almost all data you find in nature\" you can be reasonably sure that you can find a basis of eigenvectors.  For the cognescenti:  The reason is that matrices with repeated eigenvalues are the algebraic variety in $\\mathbb{C}^{(n^2)}$ which is the zero set of the discriminant of the characteristic polynomial.  The dimension of this subvariety is less than $n^2$, so it has measure $0$.  The compliment of this variety consists of all matrices which are diagonalizable with *distinct* eigenvalues.  So the set of matrices which are *not* diagonalizable with distinct eigenvalues is of measure $0$.\n",
    "\n",
    "\n",
    "We can use numpy to find eigenstuff as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the eigenvalues \n",
      " [ 2.95973016  0.37150463 -0.74754508]\n",
      "The columns of this matrix are the eigenvectors \n",
      " [[ 0.4359467   0.88306026 -0.17365211]\n",
      " [ 0.69575767 -0.45308141 -0.55734953]\n",
      " [ 0.57085177 -0.1221549   0.81191529]]\n"
     ]
    }
   ],
   "source": [
    "M = np.random.random((3,3)) \n",
    "M = M + np.transpose(M)\n",
    "    # making a random symmetric 3x3 matrix. \n",
    "    # I am making is symmetric to ensure that it has a basis of real eigenvectors.  We will learn why in a bit.\n",
    "\n",
    "eigenstuff = np.linalg.eig(M) \n",
    "    # eigenstuff is a tuple.  \n",
    "    # The index 0 element of this tuple is a numpy array of the eigenvalues.\n",
    "    # The index 1 element of the tuple is a numpy array whose columns are the eigevectors.\n",
    "    # Note the numpy normalizes the eigenvectors:  it always scales the eigenvector to have length 1.\n",
    " \n",
    "print('These are the eigenvalues \\n', np.linalg.eig(M)[0])\n",
    "print('The columns of this matrix are the eigenvectors \\n', np.linalg.eig(M)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting our knowledge of change of basis together with the definition of eigenvectors, we can say the following:\n",
    "\n",
    "**Idea**:  Let $L: \\mathbb{R}^n \\to \\mathbb{R}^n$ be a linear transformation.  Assume that $L$ has a basis $\\mathcal{B}$ of eigenvectors  (remember:  not all linear transformations *do* have such a basis).  Then the matrix of $L$ with respect to $\\mathcal{B}$ is the diagonal matrix of eigevalues.\n",
    "\n",
    "$$\n",
    "M_\\mathcal{B} = \\begin{bmatrix} \n",
    "\\lambda_1 & 0 & 0 & \\dots & 0\\\\ \n",
    "0 & \\lambda_2 & 0 & \\dots & 0\\\\ \n",
    "0 & 0 & \\lambda_3 & \\dots & 0\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ \n",
    "0 & 0 & 0 & \\dots & \\lambda_n\\\\ \n",
    "\\end{bmatrix}_\\mathcal{B}\n",
    "$$\n",
    "\n",
    "Let $B$ be the matrix whose columns are the eigenvectors and $M_L$ be the matrix of $L$ with respect to the standard basis.  Then, following our discussion about change of basis for matrices, we have\n",
    "\n",
    "$$M_\\mathcal{B} = B^{-1} M_L B$$\n",
    "\n",
    "Let's check that in numpy for a random example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is M with respect to the standard basis \n",
      " [[0.63738396 0.61657668 0.22829545]\n",
      " [0.39428364 0.59046933 0.35304557]\n",
      " [0.76772537 0.15497294 0.73634091]] \n",
      "\n",
      "This is Binverse M B \n",
      " [[ 1.484107-0.j       -0.      +0.j       -0.      -0.j      ]\n",
      " [-0.      +0.j        0.240044+0.193042j  0.      -0.j      ]\n",
      " [-0.      -0.j        0.      +0.j        0.240044-0.193042j]] \n",
      "\n",
      "This is the diagonal matrix of eigenvalues \n",
      " [[1.48410671+0.j        0.        +0.j        0.        +0.j       ]\n",
      " [0.        +0.j        0.24004375+0.1930425j 0.        +0.j       ]\n",
      " [0.        +0.j        0.        +0.j        0.24004375-0.1930425j]] \n",
      "\n",
      "They agree!\n"
     ]
    }
   ],
   "source": [
    "M = np.random.random((3,3)) \n",
    "#M = M + np.transpose(M)\n",
    "    # making a random symmetric 3x3 matrix. \n",
    "    # I am making is symmetric to ensure that it has a basis of real eigenvectors.  We will learn why in a bit.\n",
    "\n",
    "eigenstuff = np.linalg.eig(M) \n",
    "    # eigenstuff is a tuple.  \n",
    "    # The index 0 element of this tuple is a numpy array of the eigenvalues.\n",
    "    # The index 1 element of the tuple is a numpy array whose columns are the eigevectors.\n",
    "    # Note the numpy normalizes the eigenvectors:  it always scales the eigenvector to have length 1.\n",
    "B = eigenstuff[1]\n",
    "D = np.diag(eigenstuff[0]) # this makes a diagonal matrix out of the array of eigenvalues.\n",
    "Binv = np.linalg.inv(B)\n",
    "\n",
    "print('This is M with respect to the standard basis \\n', M, '\\n')\n",
    "print('This is Binverse M B \\n', np.round(np.dot(Binv, np.dot(M, B)),6),'\\n')\n",
    "print('This is the diagonal matrix of eigenvalues \\n', D, '\\n')\n",
    "print('They agree!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "What if the matrix we are working with is not a square matrix?  Can we still find \"nice\" bases for the domain and codomain so that the linear transformation \"looks like scaling the axes\" with respect to these bases?  The answer is a resounding \"yes\"!  In fact, we obtain a result which is as nice as possible:\n",
    "\n",
    "**Theorem**:  (Singular Value Decomposition = SVD) Let $L : \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation.  Then there is an orthonormal basis $\\mathcal{V} = \\left( \\vec{v}_1, \\vec{v}_2, \\vec{v}_3, \\dots, \\vec{v}_n \\right)$ of $\\mathbb{R}^n$ (called \"left singular vectors\") and an orthonormal basis $\\mathcal{U} = \\left( \\vec{u}_1, \\vec{u}_2, \\vec{u}_3, \\dots, \\vec{u}_m \\right)$ of $\\mathbb{R}^m$  (called \"right singular vectors\") and positive scalars $\\sigma_1, \\sigma_2, \\sigma_3, \\dots, \\sigma_n$ (called the \"singular values\") so that $L(\\vec{v}_j) = \\sigma_j \\vec{u}_j$.  In other words, the matrix for $L$ with respect to the bases $U$ and $V$ only has non-zero entries on the \"main diagonal\" (the entries of the matrix with equal index).\n",
    "\n",
    "This is huge!  It means that, after appropriately **rotating/reflecting** the domain and codomain independently, **every** linear map can be thought of as just independent positive scaling of the axes!\n",
    "\n",
    "A few notes and definitions:\n",
    "\n",
    "* Consider a matrix $A$ whose columns form an orthonormal basis of $\\mathbb{R}^n$.  We can think of $A^\\top A$ as computing the dot product of the columns of $A$ each other.  Since the columns are pairwize orthogonal, the dot product is $0$ unless we are dotting a column with itself (which lands us on the diagonal).  Since the columns are length $1$ we get $1$ in that case.  So $A^\\top A = I$.  Conversely, if $A^\\top A = I$, then the columns form an orthonormal basis.  We call such a matrix an **orthogonal** matrix.  Note that this means that, for orthogonal matrices only, the inverse is the same as the tranpose!\n",
    "* Using this notation of orthogonal matrices, we can rephrase the statement of the theorem as follows:\n",
    "\n",
    "**Restatement of Theorem**: Let $L : \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation.  Then there is an $n \\times n$ orthogonal matrix $V$, an $m \\times m$ orthogonal matrix $U$, and a $m \\times n$ matrix $\\Sigma$ which only has non-zero entries on the \"main diagonal\" so that\n",
    "\n",
    "$$\n",
    "\\Sigma =  U^{-1} M_L V\n",
    "$$\n",
    "\n",
    "Usually this is rewritten as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma &=  U^{-1} M_L V\\\\\n",
    "U \\Sigma &= M_L V\\\\\n",
    "U \\Sigma V^{-1} &= M_L\\\\\n",
    "M_L &= U \\Sigma V^\\top \\textrm{ since $V^{-1} = V^\\top$ for orthogonal matrices}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note:  we usually order the bases so that $\\sigma_i$ are in descending order.\n",
    "\n",
    "We can compute these matrices using Numpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      " [[-0.80586036 -0.29846487 -0.51137833]\n",
      " [-0.21918337 -0.65193721  0.72590394]\n",
      " [-0.55004339  0.69706284  0.45995182]] \n",
      "\n",
      "Sigma = \n",
      " [[1.33579627 0.        ]\n",
      " [0.         0.17543092]\n",
      " [0.         0.        ]] \n",
      "\n",
      "V = \n",
      " [[-0.87876235  0.4772596 ]\n",
      " [-0.4772596  -0.87876235]] \n",
      "\n",
      "Hopefully these two are equal: \n",
      " \n",
      " M = \n",
      " [[0.92096785 0.55976535]\n",
      " [0.20270369 0.24023813]\n",
      " [0.70402939 0.24320387]] \n",
      " \n",
      " U Sigma V-transpose = \n",
      " [[0.92096785 0.55976535]\n",
      " [0.20270369 0.24023813]\n",
      " [0.70402939 0.24320387]]\n"
     ]
    }
   ],
   "source": [
    "M = np.random.random((3,2))\n",
    "\n",
    "svdstuff = np.linalg.svd(M)\n",
    "    #svdstuff is a tuple of arrays.\n",
    "    #svdstuff[0] is the matrix U\n",
    "    #svdstuff[1] is the array of singular values.  Note:  you need to use np.diag(svdstuff[1]) to get the diagonal matrix called Sigma above.\n",
    "    #svdstuff[2] is, unfortunately, the *transpose* of the matrix V instead of being the matrix V.  Such is life.\n",
    "\n",
    "U = svdstuff[0]\n",
    "Sigma = np.zeros((3,2)) #initializing Sigma as an array of zeros with the appropriate shape (3,2).\n",
    "np.fill_diagonal(Sigma, svdstuff[1]) # Modify S in place to have diagonal entries equal singular values stored in svdstuff[1]\n",
    "V = np.transpose(svdstuff[2])\n",
    "\n",
    "print('U = \\n',U,'\\n')\n",
    "print('Sigma = \\n',Sigma,'\\n')\n",
    "print('V = \\n', V,'\\n')\n",
    "print('Hopefully these two are equal: \\n \\n M = \\n', M, '\\n \\n U Sigma V-transpose = \\n',np.dot(U, np.dot(Sigma, np.transpose(V))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that the Singular Value Decomposition is useful in data science for feature extraction.  When $X$ is an $N \\times k$ matrix recordeding $N$ observations of $k$ different variables (say, $10000$ rows each representing a person, with $5$ columns representing height, weight, blood pressure, etc ), the left singular vectors of $X$ are \"new features\" which are linear combinations of the old features, but which are orthogonal.  The left singular vectors with the largest singular values will be \"more relevant\" than left singular vectors with smaller singular values.  If we have a ton of features it is nice to be able to use SVD to do feature reduction in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1**:  Find the coordinates of $\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix}$ with respect to the basis \n",
    "> \n",
    "> $$\\mathcal{B} = \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\begin{bmatrix} 1 \\\\ -1 \\\\ 0\\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ -2\\end{bmatrix} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coordinates of \n",
      " [[1]\n",
      " [2]\n",
      " [3]] \n",
      " with respect to the columns of \n",
      " [[ 1  1  0]\n",
      " [ 1 -1  1]\n",
      " [ 1  0 -2]] \n",
      " are \n",
      " [[ 1.8]\n",
      " [-0.8]\n",
      " [-0.6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = np.array([[1],[2],[3]])\n",
    "M = np.array([[1,1,0],[1,-1,1],[1,0,-2]])\n",
    "Minv = np.linalg.inv(M)\n",
    "v_new = np.dot(Minv, v)\n",
    "print('The coordinates of \\n', v, '\\n with respect to the columns of \\n', M, '\\n are \\n', v_new )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that \n",
    "\n",
    "$$1.8\\begin{bmatrix} 1 \\\\ 1 \\\\ 1\\end{bmatrix} - 0.8\\begin{bmatrix} 1 \\\\ -1 \\\\ 0\\end{bmatrix} - 0.6\\begin{bmatrix} 0 \\\\ 1 \\\\ -2\\end{bmatrix} =  \\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix}$$\n",
    "\n",
    "So we could write\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix} 1.8 \\\\ -0.8 \\\\ -0.6\\end{bmatrix}_\\mathcal{B}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
