{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef78ced",
   "metadata": {},
   "source": [
    "# Problem Session: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b4ccc",
   "metadata": {},
   "source": [
    "Import `simulated_subject_data.csv` as `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb828f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26d6dc41",
   "metadata": {},
   "source": [
    "What are the unique values of the `subject_id` field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb5bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e23aed1",
   "metadata": {},
   "source": [
    "These data will be used in a predictive regression model.  We should approach model validation differently depending on our goals:\n",
    "\n",
    "1. Case 1:  We do not need to generalize to new `subject_id`s in the future, so we can treat `subject_id` as a categorical feature.  For instance, if each `subject_id` corresponds to a vendor we do business with, and we engage with a new vendor only very rarely, then it would be appropriate to predict a new row given knowledge of our previous history with the vendor. In this case a regular `train_test_split` and `KFold` would be appropriate.\n",
    "2. Case 2:  We do need to generalize to new `subject_id`'s in the future.  We cannot treat `subject_id` as a feature in this case.  For instance, if `subject_id` corresponds to a subject in a medical trial, and each row corresponds to a biophysical measurement under different conditions, then we want to be able to predict what a *new* subject will do under novel conditions.  We should assess our models using data splits which randomly assign different subjects to the training or holdout sets.\n",
    "\n",
    "For this problem set we will do *both* and see the difference in model performance.  This will illustrate the danger of using option 1 when you should really be using option 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a train/test split for case 1.  Use random state 216, and test size 0.2\n",
    "# Use the names X_train_1, X_test_1, y_train_1, y_test_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a train/test split for case 2. Put random 20 of the subject_ids in test, the rest in train.\n",
    "# Use the names X_train_2, X_test_2, y_train_2, y_test_2\n",
    "# There is no built-in such splitter, so you will need to write this code by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630a579",
   "metadata": {},
   "source": [
    "Let's do some EDA using `X_train_2` and `y_train_2`.  Make some graphs of the target against each feature.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb79afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c21375d",
   "metadata": {},
   "source": [
    "Let's first get a little practice with residual plots.  Fit a linear regression model to `(X_train_2, y_train_2)` using only the features $x_1, x_2, x_3$. Graph the residuals against each feature and also against the predicted values.  What do you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc4803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c56019",
   "metadata": {},
   "source": [
    "We very clearly see that we have \"un-modeled signal\" in the form of a quadratic dependence on $x_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a008b",
   "metadata": {},
   "source": [
    "We will now compare 4 different models, using the splitting strategy from Case 2:\n",
    "\n",
    "1. A baseline model which predicts the mean of the training targets.\n",
    "2. The linear regression model with features $x_1, x_2, x_3$.\n",
    "3. The linear regression model with features $x_1, x_2, x_3, x_3^2$.\n",
    "4. A random forest regression model with features $x_1, x_2, x_3$.\n",
    "\n",
    "I have included the `sklearn` packages you will need to import as a hint.  Read the docs for any you are unfamiliar with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "models = {\n",
    "    \"dummy_mean\": DummyRegressor(strategy=\"mean\"),\n",
    "    \"linear\": LinearRegression(),\n",
    "    \"linear_x3_sq\": Pipeline(\n",
    "        # your code here\n",
    "    ),\n",
    "    \"random_forest\": RandomForestRegressor(\n",
    "        n_estimators=200, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "rows = []\n",
    "for name, model in models.items():\n",
    "    # use cross_val_predict and gkf to generated out-of-fold predictions for each model.\n",
    "    # record mean out-of-fold RMSE.\n",
    "    # Also train each model on the entire training set and record in-sample RMSE.\n",
    "    # Compare in-sample to out-of-sample performance to assess overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c3dbd",
   "metadata": {},
   "source": [
    "Finally, let's see what kind of performance we would get using the quadratic model if we use `train_test_split`, `KFold`, and one-hot encode `subject_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff727bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Column transformer: x1,x2,x3 as-is, add x3^2, one-hot encode subject_id\n",
    "ct = ColumnTransformer([\n",
    "    # your code here\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"features\", ct),\n",
    "    (\"linreg\", LinearRegression())\n",
    "])\n",
    "\n",
    "# In-sample RMSE\n",
    "\n",
    "\n",
    "# Out-of-sample RMSE (row-wise KFold, no grouping)\n",
    "\n",
    "\n",
    "print(\"In-sample RMSE:\", in_rmse)\n",
    "print(\"Out-of-sample RMSE (row-wise KFold):\", out_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627d8a",
   "metadata": {},
   "source": [
    "We can see that if we were actually in Case 2, but mistakenly followed the validation strategy from Case 1, we could convince ourselves that the model performs **way better** than it actually will when applied to new subjects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
