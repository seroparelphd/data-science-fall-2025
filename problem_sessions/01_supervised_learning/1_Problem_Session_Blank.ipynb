{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Problem Session 2\n",
    "\n",
    "In this problem session we practice our skills with :\n",
    "\n",
    "* Exploratory Data Analysis\n",
    "* Simple linear regression\n",
    "* Multiple linear regression\n",
    "* k nearest neighbors regression\n",
    "* kFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## We first load in packages we will need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### 1. Introducing the data\n",
    "\n",
    "Our data concerns Median house prices for California districts derived from the 1990 census.\n",
    "\n",
    "This dataset was found on Kaggle.com, <a href=\"https://www.kaggle.com/datasets/camnugent/california-housing-prices/data\">https://www.kaggle.com/datasets/camnugent/california-housing-prices/data</a>.\n",
    "\n",
    "##### a. \n",
    "\n",
    "First load the data for this problem. It is stored in the file `housing.csv` in the `data` folder of the repository. After loading the data look at the first five rows of the dataset. Then run `housing.info()`.  Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "housing = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Yes, `total_bedrooms` has some missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "##### b. \n",
    "\n",
    "There are future lecture notebooks that cover ways to <i>impute</i> missing values, but for this notebook you will simply remove the missing values. \n",
    "\n",
    "Use `dropna`, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html</a> to get a version of the data set that has had the missing values removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "The column `median_house_value` currently contains strings instead of a floats.  Before doing any modeling you will have to clean the data a little bit.\n",
    "\n",
    "Write a function `clean_column` which passes the indicated tests. \n",
    "\n",
    "Then use `.apply`, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html</a> to apply clean_column to `median_house_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# write your function here:\n",
    "\n",
    "assert clean_column(\"$432,425.0\") == 432425.0\n",
    "assert clean_column(\"$15,326.0\") == 15326.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "housing[\"median_house_value\"] = housing[\"median_house_value\"].apply(clean_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "(d)  Look at `housing.describe()`.  Do you notice anything unusual?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "You probably noticed that our target variable appears to be **truncated** to be between $\\$14999$ and $\\$500001$.\n",
    "\n",
    "The easiest way to deal with this is to discard all of these rows.\n",
    "\n",
    "A more complicated way would be to try and utilize those rows using something like a [Tobit Model](https://en.wikipedia.org/wiki/Tobit_model).\n",
    "\n",
    "Let's take the easy way out for now.  This gives us another independent test of our model:  after training our model on the rest of the data we can see whether it predicts that those rows have a median value below $\\$15000$ or above $\\$500000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine housing to only include the rows with median house value satisfying 15000<value<500000.\n",
    "housing = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### 2. Train test split\n",
    "\n",
    "The first step in predictive modeling is performing a train test split. Perform a train test split on these data, setting aside $20\\%$ of the data as a test set. Choose a `random_state` so your results are reproducible.\n",
    "\n",
    "As a refresher you can use `sklearn`'s `train_test_split` function: \n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "\n",
    "# use train_test_split to split housing with 20% of the data as testing data.  Use a random state of 216.\n",
    "housing_train, housing_test = \n",
    "\n",
    "assert housing_train.shape[0] == int(housing.shape[0] * 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### 3. Exploratory data analysis (EDA)\n",
    "\n",
    "After the train test split we can work on some exploratory data analysis. Here is where we start to look at the data and see if we can generate any modeling ideas or hypotheses. You will make a series of plots and learn a modeling trick that should improve any models we make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### a. \n",
    "\n",
    "Use `seaborn`'s `pairplot`, <a href=\"https://seaborn.pydata.org/generated/seaborn.pairplot.html\">https://seaborn.pydata.org/generated/seaborn.pairplot.html</a> to plot `median_selling_value` against `housing_median_age`, `total_rooms`, `total_bedrooms`, `population`,`households`, and `median_income`. Shell code is provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.columns[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = housing_train.columns[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    housing_train,\n",
    "    y_vars=,\n",
    "    x_vars=,\n",
    "    height=5,\n",
    "    diag_kind=None,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Do any of the previous relationships look linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The relationship with median_income does appear roughly linear.  Hard to tell with some of the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Another part of EDA is calculating descriptive statistics.\n",
    "\n",
    "One statistic of interest to us in this situation is the <i>Pearson correlation coefficient</i>. For two variables $x$ and $y$ with $n$ observations each, the Pearson correlation is given by:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^n \\left( x_i - \\overline{x} \\right) \\left( y_i - \\overline{y}  \\right)}{\\sqrt{\\sum_{i=1}^n \\left(x_i - \\overline{x}\\right)^2 \\sum_{i=1}^n \\left(y_i - \\overline{y} \\right)^2}} = \\frac{\\text{cov}\\left(x, y\\right)}{\\sigma_x \\sigma_y},\n",
    "$$\n",
    "\n",
    "where $x_i$ is the $i^\\text{th}$ observation, $\\overline{x} = \\sum_{i=1}^n x_i/n$, $\\text{cov}\\left( x, y \\right)$ is the covariance between $x$ and $y$, and $\\sigma_x$ denotes the standard deviation of $x$.\n",
    "\n",
    "$r \\in [-1,1]$ gives a sense of the strength of the linear relationship between $x$ and $y$. The closer $|r|$ is to $1$, the stronger the linear relationship between $x$ and $y$, the sign of $r$ determines the direction of the relationship, with $r < 0$ meaning a line with a negative slope and $r > 0$ a line with a positive slope.\n",
    "\n",
    "Calculate the correlation between `median_house_value` and the columns you have previously plotted.\n",
    "\n",
    "<i>Hint: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html#pandas.DataFrame.corrwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# for your convenience I have copied the feature names here.\n",
    "# you could instead get them programmatically by slicing the housing.columns array\n",
    "\n",
    "features = [\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "]\n",
    "\n",
    "housing[features].corrwith(housing['median_house_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "Based on your EDA, which feature do you think would best predict `median_house_value` in a simple linear regression model?\n",
    "\n",
    "WARNING:  while using feature/outcome correlation is a reasonable choice for feature selection in a simple linear regression model, it is **not** a good choice for multiple linear regression.  [This stats.stackexchange post](https://stats.stackexchange.com/a/139031/97124) explains why!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "We have not yet investigated *spatial* variation in the housing prices.\n",
    "\n",
    "Use [https://plotly.com/python/mapbox-density-heatmaps/](https://plotly.com/python/mapbox-density-heatmaps/) as inspiration and make a heatmap of `median_house_value`.\n",
    "\n",
    "Does it seem like including the latitude and longitude somehow in our model would be helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### 4. Modeling\n",
    "\n",
    "Now you will build some preliminary models for this data set.\n",
    "\n",
    "##### a.\n",
    "\n",
    "When doing predictive modeling it is good practice to have a <i>baseline model</i> which is a simple \"model\" solely for comparison purposes. These are not, typically, complex or good models, but they are important reference points to give us a sense of how well our models are actually performing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Below you will use cross-validation to compare the baseline model to one simple linear regression model, one multiple linear regression model, and one kNN model which uses the spatial data.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Baseline Model}:& \\ \\text{Median House Value} = \\mathbb{E}\\left(\\text{Median House Value}\\right) + \\epsilon\\\\\n",
    "\n",
    "\\text{SLR Model}:& \\ \\text{Median House Value} = \\beta_0 + \\beta_1 \\left( \\text{Median Income} \\right) + \\epsilon\\\\\n",
    "\n",
    "\\text{MLR model}:& \\ \\text{Median House Value} = \\beta_0 + \\beta_1 \\left(\\text{Median Income}\\right)  + \\beta_2 \\left(\\text{Households}\\right) + \\epsilon\\\\\n",
    "\n",
    "\\text{kNN model}:& \\ \\text{Use k nearest neighbors regression on latitude and longitude with $k = 10$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We will attempt hyperparameter tuning on $k$ in a later problem session, but just stick with $k=10$ for now.\n",
    "\n",
    "##### b.\n",
    "In this problem practice fitting just the MLR model using the training set and `sklearn`'s `LinearRegression` model, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mlr_model = LinearRegression()\n",
    "\n",
    "mlr_model.fit(\n",
    "    housing_train[[\"median_income\", \"households\"]], housing_train.median_house_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mlr_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mlr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "In this problem you will try to implement $5$-fold cross-validation (CV) to compare these three models and the baseline model to see which one has the lowest average cross-validation root mean squared error (RMSE).\n",
    "\n",
    "Because this may be your first time implementing CV, some of the code will be filled in for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## import KFold, kNeighborsRegressor, and DummyRegressor here:\n",
    "from sklearn.model_selection import \n",
    "from sklearn.neighbors import \n",
    "from sklearn.dummy import \n",
    "\n",
    "## import root_mean_squared_error\n",
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make a KFold object\n",
    "## remember to set a random_state and set shuffle = True\n",
    "num_splits = 5\n",
    "num_models = 4\n",
    "kfold = \n",
    "\n",
    "## This array will hold the mse for each model and split\n",
    "rmses = np.zeros((num_models, num_splits))\n",
    "\n",
    "## sets a split counter\n",
    "i = 0\n",
    "\n",
    "## loop through the kfold here\n",
    "for train_index, test_index in :\n",
    "    ## cv training set\n",
    "    housing_tt = \n",
    "\n",
    "    ## cv holdout set\n",
    "    housing_ho = \n",
    "\n",
    "    ## Fit and get ho mse for the baseline model\n",
    "    mean_regressor = \n",
    "\n",
    "    ## Fit and get ho mse for slr model\n",
    "    slr = \n",
    "\n",
    "    ## Fit and get ho mse for mlr model\n",
    "    mlr = \n",
    "\n",
    "    ## Fit and get ho mse for the spatial model\n",
    "    knn = \n",
    "\n",
    "    # Record cross validation rmses\n",
    "    rmses[0, i] =\n",
    "    rmses[1, i] =\n",
    "    rmses[2, i] =\n",
    "    rmses[3, i] =\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Find the avg cv mse for each model here\n",
    "print(f\"Baseline Avg. CV RMSE: {np.mean(rmses[0,:])} and STD: {np.std(rmses[0,:])}\")\n",
    "print(f\"SLR Avg. CV MSE: {np.mean(rmses[1,:])} and STD: {np.std(rmses[1,:])}\")\n",
    "print(f\"MLR Avg. CV MSE: {np.mean(rmses[2,:])} and STD: {np.std(rmses[2,:])}\")\n",
    "print(f\"Spatial Avg. CV MSE: {np.mean(rmses[3,:])} and STD: {np.std(rmses[3,:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "Which model had the lowest average cross validation root mean squared error?  \n",
    "\n",
    "Discuss the meaning of the STD in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The spatial model performed the best, although this model is still not performing very well!  We will hopefully do better as we acquire more tools, but there are no magic bullets in machine learning.\n",
    "\n",
    "The STD of the cross validation MSEs measures the variability in the cross validation MSEs across different splits.  If this number is small then the model performance is stable with respect to variation in training data.  If this number is large, then model performance is unstable.  There are times when you might prefer a model with higher average CV MSE because it has lower variability.  For instance, imagine the following \n",
    "\n",
    "Model 1 CV MSEs:  100, 101, 99\n",
    "\n",
    "Model 2 CV MSEs:  10, 97, 190\n",
    "\n",
    "Model 1 has mean CV MSE of 100 while model 2 has mean CV MSE of 99.  If you only looked at this statistic you might choose model 2, even though model 1 is clearly superior here.  We would expect Model 1 to generalize to the test set much more reliably than model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "That's it for this notebook. In the next couple of regression based notebooks we will build additional models for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erdős Institute Data Science Boot Camp by Steven Gubkin.\n",
    "\n",
    "Please refer to the license in this repo for information on redistribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
