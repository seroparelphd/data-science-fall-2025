{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Splits for Predictive Model Comparison\n",
    "\n",
    "We want our predictive models to generalize well to unseen data. To that end we\n",
    "\n",
    "* Split our data into **training** and **testing** sets.  \n",
    "    * We never do anything with the testing set until the **very end of our work** as a final sanity check.\n",
    "* During model selection we further split our training set using either\n",
    "    * A single **validation** set or\n",
    "    * A k-fold **cross-validation** split\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the rationale for splitting our data set\n",
    "- Introduce train test splits, validation sets, and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will now start importing a common set\n",
    "## of items at the onset of most notebooks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Data splits guard against over-fitting\n",
    "\n",
    "Over-fitting is when a model fits too closely to the data it was trained on and does not generalize to new data as well as it otherwise could.\n",
    "\n",
    "We will give a more formal presentation in lecture 4 (the \"Bias/Variance Tradeoff\" notebook) but we need at least an informal understanding immediately.\n",
    "\n",
    "<img src=\"lecture_assets/overfit.png\"></img>\n",
    "\n",
    "The 2nd model is over-fitting:  we can see that it would not generalize well to new data which follows the same distribution as our training data.\n",
    "\n",
    "It was easy to see that we are over-fitting here because the relationship is relatively simple and the data is low dimensional enough that we can visualize it.  When we are dealing with real data we might have hundreds of features, and simple visual checks would not be sufficient.\n",
    "\n",
    "One of the best ways to guard against over-fitting is to use a **data split**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Train test splits\n",
    "\n",
    "The first split we will touch on is the first split you would do in a new data science project: the **train test split**.\n",
    "\n",
    "The purpose of the train test split is to create two data sets:\n",
    "1. <b>The training set</b> - This subset is used to fit models and compare model candidates. This data set is usually split further.\n",
    "2. <b>The testing set</b> - This subset is used as a final check on your selected model prior to putting your model into its desired final state.\n",
    "\n",
    "The training set usually contains the majority of the original data. Common train test split percentage divisions are $80\\% - 20\\%$ or $75\\% - 25\\%$, but it may sometimes be appropriate to use different split sizes. Train test splits are done randomly, with the form of randomness dependent upon your project.\n",
    "\n",
    "Here is an illustration of a train test split:\n",
    "\n",
    "<img src=\"lecture_assets/train_test.png\" width=\"40%\"></img>\n",
    "\n",
    "<b>IMPORTANT:  The test set is not directly used to compare models</b>\n",
    "\n",
    "Model comparison is typically done using further splits of the training set. \n",
    "\n",
    "It is embarrassing and costly to ship a product which doesn't perform well on novel data.  The test set serves as a **final sanity check** on your work before sending it out into the world.\n",
    "\n",
    "### Performing train test splits in `sklearn`\n",
    "\n",
    "The `sklearn` package has a useful `train_test_split` function that will perform the train test split. Here is a link to the documentation:\n",
    "\n",
    " <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "abalone = fetch_ucirepo(id=1)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = abalone.data.features\n",
    "y = abalone.data.targets\n",
    "\n",
    "continuous_features = [\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole_weight\",\n",
    "    \"Shucked_weight\",\n",
    "    \"Viscera_weight\",\n",
    "    \"Shell_weight\",\n",
    "]\n",
    "\n",
    "X.loc[:, continuous_features] *= 200\n",
    "y += 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we make the split\n",
    "## train_test_split returns 4 outputs: X_train, X_test, y_train and y_test\n",
    "##\n",
    "## First you input the X and y for your data\n",
    "##\n",
    "## then set the shuffle argument to True, this randomly shuffles the\n",
    "## data before it is split\n",
    "##\n",
    "## The random_state ensures that the random split is the same each time\n",
    "## someone runs the code chunk, it can be any strictly positive integer\n",
    "##\n",
    "## You can specify the size of the test set with test_size,\n",
    "## here I want 20% of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, random_state=440, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (3341, 8)\n",
      "The shape of X_test is (836, 8)\n",
      "The length of y_train is 3341\n",
      "The length of y_test is 836\n"
     ]
    }
   ],
   "source": [
    "## check the data lengths to see that they match\n",
    "## what we'd expect\n",
    "print(\"The shape of X_train is\", X_train.shape)\n",
    "print(\"The shape of X_test is\", X_test.shape)\n",
    "print(\"The length of y_train is\", len(y_train))\n",
    "print(\"The length of y_test is\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Two split types for model comparison and selection\n",
    "\n",
    "We will now cover two data splits you can make from the training set for model comparison purposes. Which you choose depends upon the project you are working on, but we will give some reasons to choose one over the other below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Validation sets\n",
    "\n",
    "A <i>validation set</i> is a subset of the training data (the result of the train test split defined above) used solely for the purpose of comparing candidate models. This split is typically also performed randomly. Further, the validation set should be a small subset, common sizes range from $10\\%-25\\%$ of the training set depending on the training set size. An illustration of this concept is given below:\n",
    "\n",
    "<img src=\"lecture_assets/validation_set.png\" width=\"45%\"></img>\n",
    "\n",
    "The best model in this setting would be the one that has the best performance metric on the validation set.\n",
    "\n",
    "#### In practice\n",
    "\n",
    "In practice we can once again use `sklearn`'s `train_test_split` function to make the validation split. Note that it is good practice to not overwrite the original `X_train` or `y_train` sets when making the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we make a validation set with 15% of the\n",
    "## training data in the validation set\n",
    "X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "    X_train, y_train, shuffle=True, random_state=321, test_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15% of 3341 is 501.15\n"
     ]
    }
   ],
   "source": [
    "print(\"15% of\", len(X_train), \"is\", 0.15 * len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_train (2839, 8)\n",
      "Shape of X_val (502, 8)\n",
      "Length of y_train_train 2839\n",
      "Length of y_val 502\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_train\", X_train_train.shape)\n",
    "print(\"Shape of X_val\", X_val.shape)\n",
    "print(\"Length of y_train_train\", len(y_train_train))\n",
    "print(\"Length of y_val\", len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### $k$-Fold cross-validation\n",
    "\n",
    "The validation set approach only gives us one check on how well our model generalizes.  We might get unusually lucky or unlucky with this one check.  $k$-fold cross validation gives us $k$ opportunities to see how well our model will generalize instead of just one.\n",
    "\n",
    "<img src=\"lecture_assets/cv1.png\" width=\"60%\"></img>\n",
    "\n",
    "Common values for $k$ are between $5$ and $10$.  \"Leave out one\" cross validation is another strategy which is equivalent to taking $k = n-1$ where $n$ is the number of samples in your training data.\n",
    "\n",
    "You can implement cross-validation using `sklearn`'s `KFold` object. Documentation for this method can be found here <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import KFold\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a KFold object\n",
    "## n_splits controls the value of k\n",
    "## shuffle=True, randomly shuffles the data prior to splitting\n",
    "## random_state is the same as for train_test_split\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x16928ef00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demonstrate.split\n",
    "kfold.split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Side note on generators:  Notice that kfold.split returns a generator object.  If you are not familiar with them, you can think of this as being similar to a list except that instead of storing all of the elements in memory it stores the current element and a rule for getting the next element.\n",
    "\n",
    "KFold is implemented this way to deal with memory issues if you use a large number of splits.  For instance, if a Leave Out One split was implemented as a list on a dataset of size $10000$ the size of the list would be $10000*9999$.  If you use a generator instead then at each stage you only need to keep a list of size $10000$ in memory, also remember which element you should leave out next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train index: [   0    2    4 ... 3337 3339 3340]\n",
      "Test index: [   1    3   18   19   40   41   42   44   46   47   48   52   59   64\n",
      "   65   72   78   79   80   91   96  114  118  122  123  128  129  141\n",
      "  142  144  152  156  160  167  168  172  176  183  184  187  193  199\n",
      "  214  218  225  232  235  247  256  264  267  270  275  283  286  287\n",
      "  304  311  320  321  322  324  325  326  336  341  349  352  354  369\n",
      "  370  371  373  383  385  389  390  397  403  410  413  416  431  442\n",
      "  444  445  449  450  454  464  465  467  476  486  487  491  495  496\n",
      "  500  504  508  518  529  535  536  538  539  544  549  552  553  565\n",
      "  567  568  573  575  583  595  607  613  618  619  622  625  631  633\n",
      "  638  643  644  648  649  651  654  659  663  672  673  674  677  679\n",
      "  682  685  692  705  720  727  729  732  737  747  769  773  780  783\n",
      "  785  791  796  797  800  801  815  820  822  831  834  836  838  840\n",
      "  841  842  845  846  851  860  863  872  875  883  885  889  895  903\n",
      "  910  912  913  924  926  929  932  933  938  940  943  951  952  953\n",
      "  959  966  969  977  991  992  996 1008 1011 1012 1019 1025 1027 1036\n",
      " 1045 1046 1051 1052 1054 1057 1065 1068 1072 1076 1083 1084 1089 1096\n",
      " 1105 1106 1114 1115 1116 1119 1124 1137 1139 1147 1163 1164 1167 1168\n",
      " 1170 1178 1181 1182 1190 1199 1201 1202 1220 1232 1243 1250 1253 1255\n",
      " 1258 1259 1264 1265 1268 1275 1278 1279 1281 1284 1293 1297 1300 1301\n",
      " 1306 1307 1310 1319 1324 1331 1342 1344 1346 1350 1355 1360 1365 1367\n",
      " 1371 1374 1379 1380 1381 1390 1395 1404 1416 1419 1426 1428 1432 1440\n",
      " 1444 1445 1448 1452 1454 1455 1459 1461 1462 1463 1465 1472 1473 1475\n",
      " 1480 1481 1484 1486 1495 1506 1519 1521 1527 1528 1529 1531 1536 1539\n",
      " 1541 1552 1555 1573 1574 1577 1579 1582 1587 1592 1593 1597 1599 1605\n",
      " 1608 1613 1621 1636 1641 1642 1648 1649 1650 1651 1656 1661 1665 1676\n",
      " 1682 1687 1688 1697 1704 1705 1706 1709 1715 1718 1724 1725 1726 1733\n",
      " 1746 1751 1755 1761 1762 1770 1773 1778 1782 1784 1790 1796 1802 1804\n",
      " 1827 1830 1831 1835 1843 1848 1856 1859 1861 1867 1869 1874 1875 1878\n",
      " 1881 1882 1885 1892 1905 1914 1918 1920 1927 1929 1935 1946 1963 1971\n",
      " 1972 1984 1991 2003 2016 2026 2036 2043 2056 2057 2060 2061 2064 2065\n",
      " 2066 2068 2085 2088 2093 2099 2102 2105 2108 2117 2124 2129 2132 2134\n",
      " 2137 2142 2155 2157 2164 2167 2170 2171 2179 2180 2181 2183 2185 2194\n",
      " 2202 2205 2212 2216 2218 2219 2220 2229 2233 2234 2236 2237 2241 2242\n",
      " 2244 2247 2254 2255 2257 2259 2268 2274 2278 2279 2283 2285 2286 2292\n",
      " 2293 2297 2300 2310 2312 2316 2317 2322 2323 2325 2329 2337 2339 2343\n",
      " 2344 2348 2351 2353 2360 2362 2371 2377 2379 2382 2387 2390 2393 2400\n",
      " 2407 2409 2414 2416 2424 2425 2431 2433 2438 2445 2447 2454 2457 2459\n",
      " 2460 2465 2466 2470 2490 2491 2492 2493 2494 2496 2500 2501 2504 2510\n",
      " 2513 2532 2536 2542 2545 2547 2565 2568 2573 2575 2576 2581 2584 2588\n",
      " 2598 2603 2608 2614 2617 2622 2623 2625 2627 2630 2632 2639 2641 2643\n",
      " 2646 2651 2653 2663 2666 2667 2685 2686 2693 2703 2708 2709 2711 2712\n",
      " 2713 2726 2728 2732 2733 2734 2739 2740 2742 2747 2750 2752 2765 2771\n",
      " 2784 2813 2823 2840 2841 2849 2857 2878 2881 2886 2887 2897 2901 2905\n",
      " 2906 2909 2924 2931 2935 2937 2939 2944 2955 2958 2962 2967 2974 2982\n",
      " 2984 2985 2999 3000 3011 3028 3030 3032 3037 3046 3051 3056 3061 3069\n",
      " 3072 3074 3076 3090 3106 3123 3139 3142 3149 3165 3167 3180 3182 3183\n",
      " 3184 3185 3189 3190 3191 3193 3197 3198 3199 3203 3216 3235 3241 3255\n",
      " 3257 3264 3265 3291 3304 3313 3314 3324 3326 3333 3338]\n",
      "\n",
      "\n",
      "Train index: [   1    2    3 ... 3338 3339 3340]\n",
      "Test index: [   0    8   10   12   21   25   29   31   33   57   70   74   76   77\n",
      "   82   83   99  100  101  106  107  121  133  138  140  150  169  178\n",
      "  180  195  197  202  205  209  211  213  215  228  231  234  239  244\n",
      "  245  260  263  265  269  272  274  277  280  281  285  292  293  295\n",
      "  296  299  301  307  317  328  329  330  332  338  345  355  372  384\n",
      "  388  392  398  402  404  407  415  418  426  428  429  435  436  438\n",
      "  439  458  460  462  472  477  480  485  515  517  519  522  523  525\n",
      "  532  543  550  551  555  556  557  558  570  577  578  582  586  588\n",
      "  592  596  599  602  604  608  612  620  621  634  645  656  668  680\n",
      "  688  693  696  698  701  702  704  706  708  709  710  711  713  717\n",
      "  721  722  726  745  746  755  761  764  765  766  767  771  784  787\n",
      "  799  805  808  814  818  819  832  837  839  850  861  865  867  868\n",
      "  887  897  899  901  908  911  915  919  927  931  934  936  942  946\n",
      "  962  968  974  984  987  989  998  999 1007 1017 1022 1023 1028 1031\n",
      " 1035 1040 1043 1044 1053 1056 1059 1063 1064 1069 1079 1080 1091 1092\n",
      " 1094 1104 1108 1112 1118 1121 1123 1125 1131 1132 1144 1145 1152 1155\n",
      " 1156 1157 1165 1176 1187 1196 1198 1204 1205 1206 1211 1224 1227 1231\n",
      " 1239 1245 1248 1267 1270 1272 1276 1283 1287 1288 1294 1303 1321 1325\n",
      " 1326 1327 1329 1330 1338 1341 1351 1354 1369 1372 1375 1383 1385 1387\n",
      " 1394 1397 1409 1410 1411 1413 1417 1418 1420 1421 1425 1427 1451 1464\n",
      " 1466 1467 1478 1482 1485 1488 1493 1494 1498 1499 1501 1502 1507 1511\n",
      " 1516 1517 1520 1524 1533 1534 1537 1544 1551 1553 1556 1569 1598 1600\n",
      " 1603 1612 1614 1616 1623 1625 1626 1628 1637 1638 1639 1643 1657 1658\n",
      " 1662 1667 1675 1677 1678 1686 1689 1694 1696 1699 1702 1710 1717 1721\n",
      " 1727 1728 1729 1736 1739 1745 1754 1756 1768 1780 1785 1786 1788 1791\n",
      " 1793 1794 1809 1810 1837 1838 1844 1845 1865 1871 1879 1888 1891 1895\n",
      " 1900 1903 1908 1917 1926 1939 1940 1942 1943 1945 1948 1950 1952 1957\n",
      " 1961 1966 1977 1981 1982 1989 1992 1995 2006 2009 2010 2012 2013 2017\n",
      " 2018 2022 2028 2032 2033 2040 2048 2050 2071 2072 2075 2081 2083 2084\n",
      " 2090 2091 2095 2106 2109 2118 2121 2133 2135 2139 2149 2177 2178 2184\n",
      " 2187 2191 2192 2196 2203 2208 2221 2222 2248 2250 2253 2263 2275 2276\n",
      " 2282 2287 2295 2296 2299 2301 2304 2307 2311 2314 2318 2320 2340 2352\n",
      " 2361 2367 2381 2383 2385 2388 2396 2397 2399 2404 2410 2413 2420 2427\n",
      " 2429 2430 2432 2434 2442 2455 2461 2463 2467 2474 2475 2487 2495 2497\n",
      " 2498 2503 2505 2508 2509 2516 2517 2519 2520 2521 2522 2523 2524 2529\n",
      " 2530 2533 2535 2543 2544 2549 2552 2555 2557 2560 2563 2566 2569 2571\n",
      " 2574 2578 2585 2587 2601 2604 2606 2611 2619 2624 2628 2633 2640 2642\n",
      " 2649 2652 2657 2670 2677 2682 2684 2688 2690 2691 2694 2696 2698 2700\n",
      " 2701 2706 2715 2716 2718 2719 2725 2729 2730 2738 2748 2756 2762 2763\n",
      " 2767 2768 2778 2780 2794 2796 2799 2807 2808 2809 2810 2812 2814 2820\n",
      " 2830 2831 2837 2839 2843 2845 2846 2847 2848 2854 2859 2866 2868 2870\n",
      " 2873 2879 2902 2904 2908 2911 2916 2923 2926 2930 2945 2946 2956 2959\n",
      " 2961 2965 2970 2979 2981 2992 3010 3014 3016 3018 3019 3020 3023 3025\n",
      " 3040 3050 3057 3062 3063 3064 3070 3071 3075 3079 3083 3084 3086 3088\n",
      " 3097 3101 3111 3120 3126 3128 3130 3134 3141 3148 3162 3163 3170 3171\n",
      " 3175 3181 3192 3196 3204 3206 3207 3212 3219 3223 3229 3230 3231 3232\n",
      " 3239 3242 3249 3252 3253 3254 3256 3258 3259 3268 3276 3277 3283 3289\n",
      " 3305 3307 3308 3310 3312 3316 3322 3327 3331 3335]\n",
      "\n",
      "\n",
      "Train index: [   0    1    2 ... 3338 3339 3340]\n",
      "Test index: [   9   16   20   26   28   30   32   35   37   39   43   56   62   67\n",
      "   71   81   84   90   97  113  119  124  136  143  148  149  153  158\n",
      "  165  166  173  177  182  191  194  200  201  203  204  207  210  217\n",
      "  219  224  230  236  237  238  240  241  246  251  254  258  261  266\n",
      "  279  284  288  294  298  302  303  305  308  318  333  350  358  365\n",
      "  367  374  381  393  395  405  406  409  414  422  433  440  441  447\n",
      "  448  452  453  455  459  468  469  474  481  482  488  490  497  501\n",
      "  502  503  512  513  514  521  526  528  540  560  562  581  584  587\n",
      "  593  598  606  609  610  614  627  637  639  640  641  646  650  652\n",
      "  653  658  660  664  666  667  670  675  700  707  712  714  725  733\n",
      "  738  744  750  752  753  756  757  763  774  775  779  792  794  802\n",
      "  804  806  811  816  817  821  825  826  828  829  830  849  852  853\n",
      "  859  864  866  870  874  877  879  882  884  886  888  892  893  907\n",
      "  923  930  935  937  941  948  949  954  957  963  965  971  972  975\n",
      "  980  983  985  990  994 1000 1015 1039 1047 1048 1061 1073 1074 1075\n",
      " 1082 1086 1095 1100 1101 1113 1120 1122 1127 1128 1129 1130 1134 1135\n",
      " 1140 1142 1149 1151 1158 1162 1173 1177 1183 1184 1186 1195 1197 1203\n",
      " 1209 1210 1212 1217 1218 1223 1228 1236 1241 1246 1247 1251 1252 1254\n",
      " 1260 1262 1274 1280 1282 1285 1295 1296 1298 1304 1313 1315 1316 1317\n",
      " 1318 1322 1323 1334 1339 1349 1353 1359 1361 1362 1363 1364 1373 1376\n",
      " 1384 1386 1391 1398 1401 1406 1407 1408 1412 1414 1424 1429 1436 1437\n",
      " 1439 1449 1450 1456 1458 1468 1471 1489 1490 1492 1496 1497 1505 1510\n",
      " 1518 1532 1538 1540 1545 1550 1559 1560 1564 1565 1568 1570 1576 1583\n",
      " 1590 1594 1601 1602 1607 1615 1627 1630 1631 1644 1645 1653 1654 1655\n",
      " 1660 1663 1666 1672 1680 1683 1690 1691 1693 1698 1700 1703 1712 1720\n",
      " 1732 1735 1743 1744 1747 1759 1760 1764 1765 1766 1769 1777 1779 1792\n",
      " 1798 1803 1812 1816 1817 1826 1828 1832 1834 1839 1840 1847 1850 1852\n",
      " 1857 1862 1872 1876 1886 1894 1897 1899 1901 1902 1904 1909 1915 1922\n",
      " 1924 1928 1930 1931 1936 1938 1941 1947 1964 1970 1996 2000 2005 2007\n",
      " 2011 2014 2020 2021 2024 2029 2031 2037 2038 2039 2047 2049 2051 2053\n",
      " 2062 2069 2078 2087 2092 2096 2097 2128 2130 2131 2141 2143 2144 2148\n",
      " 2150 2154 2159 2165 2168 2188 2190 2193 2197 2198 2201 2204 2213 2217\n",
      " 2224 2231 2245 2246 2249 2252 2261 2262 2270 2271 2272 2289 2298 2306\n",
      " 2313 2327 2333 2342 2346 2350 2354 2355 2356 2368 2376 2386 2389 2392\n",
      " 2398 2402 2406 2408 2411 2412 2415 2422 2426 2435 2443 2446 2448 2452\n",
      " 2464 2471 2472 2486 2507 2512 2525 2527 2541 2546 2553 2554 2564 2572\n",
      " 2577 2582 2583 2591 2595 2596 2599 2610 2613 2615 2618 2621 2629 2631\n",
      " 2635 2636 2654 2658 2660 2662 2664 2669 2680 2681 2683 2689 2692 2702\n",
      " 2714 2720 2722 2723 2744 2745 2749 2754 2755 2759 2769 2779 2785 2786\n",
      " 2790 2801 2805 2806 2816 2821 2822 2824 2825 2828 2833 2834 2835 2844\n",
      " 2862 2869 2876 2877 2885 2888 2889 2892 2894 2898 2900 2907 2910 2912\n",
      " 2917 2921 2927 2928 2932 2940 2941 2942 2947 2957 2960 2968 2973 2975\n",
      " 2977 2986 2991 2994 2995 2996 2997 2998 3012 3015 3017 3022 3024 3026\n",
      " 3034 3049 3052 3055 3058 3059 3065 3082 3087 3091 3092 3093 3095 3099\n",
      " 3100 3102 3104 3107 3114 3117 3121 3125 3131 3138 3140 3150 3156 3157\n",
      " 3158 3159 3161 3168 3172 3173 3194 3201 3208 3211 3214 3215 3218 3220\n",
      " 3225 3227 3234 3237 3240 3243 3247 3248 3262 3263 3267 3273 3274 3275\n",
      " 3278 3281 3285 3293 3299 3302 3323 3325 3329 3337]\n",
      "\n",
      "\n",
      "Train index: [   0    1    3 ... 3337 3338 3339]\n",
      "Test index: [   2    4    7   17   24   36   45   50   51   53   63   66   68   73\n",
      "   75   85   94   98  102  104  105  109  110  117  125  126  130  131\n",
      "  132  134  147  162  175  179  181  185  186  188  190  192  196  206\n",
      "  212  220  222  223  226  227  229  233  242  248  250  252  253  255\n",
      "  259  262  268  276  278  289  290  309  312  314  315  323  334  337\n",
      "  343  348  351  353  356  357  359  360  362  363  364  368  376  377\n",
      "  378  379  396  399  400  411  420  421  423  424  425  430  432  437\n",
      "  443  446  463  470  471  473  475  483  493  494  498  505  506  509\n",
      "  510  527  534  547  548  554  559  561  566  569  571  572  574  580\n",
      "  589  591  594  597  600  601  603  605  615  617  623  624  626  630\n",
      "  635  636  655  661  665  669  671  676  678  681  683  684  687  691\n",
      "  703  718  724  734  735  740  749  751  754  758  759  760  762  776\n",
      "  781  782  786  788  793  795  803  807  809  810  812  823  843  844\n",
      "  848  855  856  858  862  871  878  890  894  900  902  906  909  914\n",
      "  920  922  939  944  945  955  956  958  960  964  970  981  995  997\n",
      " 1004 1009 1010 1013 1014 1018 1029 1030 1033 1037 1038 1050 1066 1067\n",
      " 1070 1071 1087 1098 1099 1109 1126 1146 1161 1174 1175 1179 1188 1189\n",
      " 1191 1208 1213 1214 1216 1221 1222 1226 1229 1233 1234 1238 1242 1249\n",
      " 1256 1257 1261 1277 1286 1289 1290 1299 1305 1312 1314 1320 1332 1333\n",
      " 1337 1340 1343 1345 1347 1348 1356 1368 1370 1388 1389 1396 1400 1415\n",
      " 1422 1423 1431 1433 1438 1447 1453 1469 1470 1474 1477 1483 1491 1504\n",
      " 1508 1509 1513 1514 1515 1525 1526 1530 1535 1542 1543 1547 1549 1558\n",
      " 1562 1563 1566 1567 1578 1580 1584 1588 1589 1596 1604 1606 1609 1610\n",
      " 1611 1617 1618 1632 1633 1634 1635 1640 1647 1652 1659 1668 1670 1673\n",
      " 1674 1679 1681 1685 1695 1707 1708 1711 1713 1723 1731 1734 1737 1740\n",
      " 1742 1748 1749 1757 1767 1771 1772 1774 1775 1776 1783 1787 1789 1797\n",
      " 1799 1801 1805 1806 1807 1808 1813 1814 1815 1818 1819 1821 1822 1823\n",
      " 1836 1842 1853 1854 1860 1863 1864 1866 1868 1877 1883 1884 1890 1896\n",
      " 1906 1907 1911 1932 1933 1944 1951 1953 1954 1955 1959 1960 1973 1975\n",
      " 1979 1980 1985 1986 1990 1993 1994 1997 1999 2004 2008 2019 2027 2035\n",
      " 2041 2046 2052 2054 2055 2058 2059 2063 2067 2073 2074 2077 2082 2089\n",
      " 2094 2101 2104 2110 2112 2113 2114 2116 2119 2120 2125 2127 2138 2140\n",
      " 2146 2151 2158 2161 2162 2166 2172 2182 2186 2195 2200 2207 2209 2215\n",
      " 2227 2230 2239 2240 2251 2260 2264 2265 2266 2269 2273 2280 2294 2305\n",
      " 2308 2319 2321 2324 2326 2328 2330 2331 2338 2345 2347 2357 2358 2363\n",
      " 2364 2365 2366 2370 2373 2375 2391 2394 2395 2403 2418 2421 2423 2436\n",
      " 2437 2439 2441 2450 2453 2456 2458 2462 2468 2469 2476 2477 2480 2484\n",
      " 2485 2489 2511 2514 2515 2518 2526 2528 2538 2540 2550 2551 2558 2559\n",
      " 2561 2562 2567 2570 2579 2580 2586 2590 2600 2602 2612 2626 2647 2648\n",
      " 2655 2656 2661 2671 2674 2679 2697 2699 2717 2735 2736 2737 2741 2746\n",
      " 2757 2777 2782 2783 2788 2789 2792 2793 2795 2798 2804 2811 2818 2832\n",
      " 2838 2851 2852 2856 2858 2861 2864 2865 2867 2871 2874 2875 2880 2882\n",
      " 2883 2884 2903 2914 2919 2922 2925 2929 2938 2949 2952 2964 2972 2976\n",
      " 2987 2989 2993 3007 3009 3013 3029 3031 3033 3035 3036 3038 3044 3045\n",
      " 3047 3053 3066 3073 3078 3081 3085 3096 3105 3108 3110 3112 3124 3129\n",
      " 3132 3133 3137 3147 3151 3155 3169 3177 3178 3179 3202 3205 3221 3224\n",
      " 3236 3238 3244 3246 3250 3260 3270 3271 3272 3279 3288 3290 3297 3301\n",
      " 3303 3306 3311 3315 3317 3318 3319 3320 3334 3340]\n",
      "\n",
      "\n",
      "Train index: [   0    1    2 ... 3337 3338 3340]\n",
      "Test index: [   5    6   11   13   14   15   22   23   27   34   38   49   54   55\n",
      "   58   60   61   69   86   87   88   89   92   93   95  103  108  111\n",
      "  112  115  116  120  127  135  137  139  145  146  151  154  155  157\n",
      "  159  161  163  164  170  171  174  189  198  208  216  221  243  249\n",
      "  257  271  273  282  291  297  300  306  310  313  316  319  327  331\n",
      "  335  339  340  342  344  346  347  361  366  375  380  382  386  387\n",
      "  391  394  401  408  412  417  419  427  434  451  456  457  461  466\n",
      "  478  479  484  489  492  499  507  511  516  520  524  530  531  533\n",
      "  537  541  542  545  546  563  564  576  579  585  590  611  616  628\n",
      "  629  632  642  647  657  662  686  689  690  694  695  697  699  715\n",
      "  716  719  723  728  730  731  736  739  741  742  743  748  768  770\n",
      "  772  777  778  789  790  798  813  824  827  833  835  847  854  857\n",
      "  869  873  876  880  881  891  896  898  904  905  916  917  918  921\n",
      "  925  928  947  950  961  967  973  976  978  979  982  986  988  993\n",
      " 1001 1002 1003 1005 1006 1016 1020 1021 1024 1026 1032 1034 1041 1042\n",
      " 1049 1055 1058 1060 1062 1077 1078 1081 1085 1088 1090 1093 1097 1102\n",
      " 1103 1107 1110 1111 1117 1133 1136 1138 1141 1143 1148 1150 1153 1154\n",
      " 1159 1160 1166 1169 1171 1172 1180 1185 1192 1193 1194 1200 1207 1215\n",
      " 1219 1225 1230 1235 1237 1240 1244 1263 1266 1269 1271 1273 1291 1292\n",
      " 1302 1308 1309 1311 1328 1335 1336 1352 1357 1358 1366 1377 1378 1382\n",
      " 1392 1393 1399 1402 1403 1405 1430 1434 1435 1441 1442 1443 1446 1457\n",
      " 1460 1476 1479 1487 1500 1503 1512 1522 1523 1546 1548 1554 1557 1561\n",
      " 1571 1572 1575 1581 1585 1586 1591 1595 1619 1620 1622 1624 1629 1646\n",
      " 1664 1669 1671 1684 1692 1701 1714 1716 1719 1722 1730 1738 1741 1750\n",
      " 1752 1753 1758 1763 1781 1795 1800 1811 1820 1824 1825 1829 1833 1841\n",
      " 1846 1849 1851 1855 1858 1870 1873 1880 1887 1889 1893 1898 1910 1912\n",
      " 1913 1916 1919 1921 1923 1925 1934 1937 1949 1956 1958 1962 1965 1967\n",
      " 1968 1969 1974 1976 1978 1983 1987 1988 1998 2001 2002 2015 2023 2025\n",
      " 2030 2034 2042 2044 2045 2070 2076 2079 2080 2086 2098 2100 2103 2107\n",
      " 2111 2115 2122 2123 2126 2136 2145 2147 2152 2153 2156 2160 2163 2169\n",
      " 2173 2174 2175 2176 2189 2199 2206 2210 2211 2214 2223 2225 2226 2228\n",
      " 2232 2235 2238 2243 2256 2258 2267 2277 2281 2284 2288 2290 2291 2302\n",
      " 2303 2309 2315 2332 2334 2335 2336 2341 2349 2359 2369 2372 2374 2378\n",
      " 2380 2384 2401 2405 2417 2419 2428 2440 2444 2449 2451 2473 2478 2479\n",
      " 2481 2482 2483 2488 2499 2502 2506 2531 2534 2537 2539 2548 2556 2589\n",
      " 2592 2593 2594 2597 2605 2607 2609 2616 2620 2634 2637 2638 2644 2645\n",
      " 2650 2659 2665 2668 2672 2673 2675 2676 2678 2687 2695 2704 2705 2707\n",
      " 2710 2721 2724 2727 2731 2743 2751 2753 2758 2760 2761 2764 2766 2770\n",
      " 2772 2773 2774 2775 2776 2781 2787 2791 2797 2800 2802 2803 2815 2817\n",
      " 2819 2826 2827 2829 2836 2842 2850 2853 2855 2860 2863 2872 2890 2891\n",
      " 2893 2895 2896 2899 2913 2915 2918 2920 2933 2934 2936 2943 2948 2950\n",
      " 2951 2953 2954 2963 2966 2969 2971 2978 2980 2983 2988 2990 3001 3002\n",
      " 3003 3004 3005 3006 3008 3021 3027 3039 3041 3042 3043 3048 3054 3060\n",
      " 3067 3068 3077 3080 3089 3094 3098 3103 3109 3113 3115 3116 3118 3119\n",
      " 3122 3127 3135 3136 3143 3144 3145 3146 3152 3153 3154 3160 3164 3166\n",
      " 3174 3176 3186 3187 3188 3195 3200 3209 3210 3213 3217 3222 3226 3228\n",
      " 3233 3245 3251 3261 3266 3269 3280 3282 3284 3286 3287 3292 3294 3295\n",
      " 3296 3298 3300 3309 3321 3328 3330 3332 3336 3339]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## use for loop to demonstrate .split\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    print(\"Train index:\", train_index)\n",
    "    print(\"Test index:\", test_index)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Choosing between our two models using cross validation\n",
    "\n",
    "We have two predictive models of abalone age:  one which only uses length as a predictor, and one which uses all of the continuous predictors.  Let's assess which of these two models generalizes the best to unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "slr = LinearRegression()\n",
    "mlr = LinearRegression()\n",
    "\n",
    "# rmses will hold the cross validation root mean squared errors of each model.\n",
    "rmses = np.zeros((2, 5))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    ## get the kfold training data\n",
    "    X_train_train = X_train.iloc[train_index, :]\n",
    "    y_train_train = y_train.iloc[train_index]\n",
    "\n",
    "    ## get the holdout data\n",
    "    X_holdout = X_train.iloc[test_index, :]\n",
    "    y_holdout = y_train.iloc[test_index]\n",
    "\n",
    "    ## Fit both models\n",
    "    slr.fit(X_train_train[[\"Length\"]], y_train_train)\n",
    "    mlr.fit(X_train_train[continuous_features], y_train_train)\n",
    "\n",
    "    ## Use both models to generate predictions on the holdout set\n",
    "    slr_preds = slr.predict(X_holdout[[\"Length\"]])\n",
    "    mlr_preds = mlr.predict(X_holdout[continuous_features])\n",
    "\n",
    "    ## Record the rmses\n",
    "    rmses[0, i] = root_mean_squared_error(y_holdout, slr_preds)\n",
    "    rmses[1, i] = root_mean_squared_error(y_holdout, mlr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.73941346, 2.63849335, 2.50858008, 2.86749772, 2.60895888],\n",
       "       [2.46317672, 2.16690161, 2.07821673, 2.39625422, 2.18157873]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6725887, 2.2572256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmses.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We can see that the multiple linear regression model does generalize a bit better.  Depending on our use case this improvement may, or may not, be worth it.  For example, it will be easier for an abalone diver to measure one length than it is to collect all $7$ features in our multiple linear regression model.\n",
    "\n",
    "If we were done modeling we could then do our \"final sanity check\" on the testing data.  We would retrain our chosen model on the entire training set and then evaluate on the test set. In this case I want to do some more modeling using the same dataset in the coming weeks.  So we will hold off on evaluating the model on the testing data for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Validation set or cross-validation\n",
    "\n",
    "Cross-validation, when feasible, is preferred to a single validation set. In general it is better to have a collection of estimates of the generalization error instead of a single point estimate.\n",
    "\n",
    "However, it is not always feasible to perform cross-validation. Models that take prohibitively long to train limit the usefulness of cross-validation since $k$-fold cross-validation requires you to train the model $k$ distinct times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff676b",
   "metadata": {},
   "source": [
    "### Additional Considerations\n",
    "\n",
    "It is very rare that a simple `train_test_split` or `KFold` will be the appropriate data split for your problem. You need to think carefully about exactly what information you will have access to at prediction time, and what new situations you would like to be able to generalize to.  You need to make sure that your split respects those considerations.\n",
    "\n",
    "- **Geographic leakage**  \n",
    "  - Nearby or correlated spatial units split across train/test.  \n",
    "  - Example: adjacent houses, stores in the same shopping center, patients from the same hospital.  \n",
    "  - Tools: manual grouping, [`GroupKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html)\n",
    "    - `GroupKFold` alone is often insufficient:  custom splitters or splitting pipelines (e.g. first use spatial clustering techniques to assign groups, then use `GroupKFold`). \n",
    "- **Temporal leakage**  \n",
    "  - Future information leaking into past predictions.  \n",
    "  - Example: using 2023 features to predict 2022 outcomes.  \n",
    "  - Tools: [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)\n",
    "    - `TimeSeriesSplit` is often not the correct structure and you will need to write a custom splitter.  For example, if data only becomes available one week before predictions are needed it is inappropriate to train on all data up to the prediction time.\n",
    "- **Group-level leakage**  \n",
    "  - Data about the same entity split across train/test.  \n",
    "  - Example: Suppose you want to predict how a new student will perform on a standardized test at a new school. Your rows are individual student test events. If you use train_test_split or KFold, students from the same school end up in both train and test sets, so you never measure generalization to unseen schools. The fix is to split at the school level.\n",
    "  - Tools: [`GroupKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html), [`GroupShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html), [`LeaveOneGroupOut`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html)\n",
    "- **Feature leakage**  \n",
    "  - Variables that encode target information directly.\n",
    "- **Post-outcome leakage**  \n",
    "  - Including features that were only known after the prediction time.  \n",
    "  - No direct tool; requires domain awareness.\n",
    "- **Overfitting the split**  \n",
    "  - Repeatedly adjusting models or features to perform well on a single validation/test split risks overfitting that particular partition rather than learning patterns that generalize.  \n",
    "  - Mitigation: use multiple splits (e.g., cross-validation, nested CV).  Always keep a truly final holdout set untouched until the very end.\n",
    "  - You should only ever evaluate **one model** on the final test set: it isn't another validation set.\n",
    "- **IMPORTANT NOTE**\n",
    "  - In many cases, the built-in splitters won’t match your problem. You may need to implement a custom splitting strategy that explicitly mirrors the situations where you expect the model to generalize. Think carefully about the real deployment setting, and design your split to reflect it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
