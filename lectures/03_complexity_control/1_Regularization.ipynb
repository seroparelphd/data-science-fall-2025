{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd7c765",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization involves adding a penalty term to our loss function. It turns out that this penalty term can help combat overfitting by making the model more biased but with less variance.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the general idea behind regularization,\n",
    "- Discuss ridge and lasso regression as particular regularization algorithms\n",
    "- Discuss how ridge can combat multicollinearity.\n",
    "- Show how lasso is nice for feature selection.\n",
    "\n",
    "##### Quick Note\n",
    "\n",
    "This notebook is a little math heavy, I will do my best to provide both mathematical insight for those that want it and give a broad overview for those that do not want to delve too much into the math specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d62ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d754519",
   "metadata": {},
   "source": [
    "## The idea behind regularization\n",
    "\n",
    "Recall our supervised learning framework:\n",
    "\n",
    "A **model** or **hypothesis class** is a collection of functions $\\mathcal{H} = \\{f_\\theta: \\mathbb{R}^p \\to \\mathbb{R^m}: \\theta \\in \\mathcal{\\Theta} \\subset \\mathbb{R}^q\\}$.\n",
    "\n",
    "We need a way to evaluate the performance of each $f_\\theta$ at reproducing the input/output pairs we actually observed in the data. This performance evaluation metric is called a **loss function**.  It will a single real number score $\\ell(\\theta)$ to each parameter vector $\\theta$.\n",
    "\n",
    "We *fit* the model by finding the parameters which minimize $\\ell(\\theta)$.\n",
    "\n",
    "To summarize to do supervised learning we need\n",
    "\n",
    "* Data which comes in pairs of input measurements and output measurements $(\\vec{x}_i, y_i)$ for $i = 1, 2, 3, \\dots, n$.\n",
    "* We select a *model* $f_\\theta$ which is the collection of functions we will consider as candidates and is parameterized by some list of numbers $\\theta$.\n",
    "* We select a *loss function* $\\ell$ which allows us to judge how any particular $f_\\theta$ performs on our data.\n",
    "* We minimize the loss function to obtain the *fitted model* $\\hat{f}$ which has parameters $\\hat{\\theta}$.\n",
    "    * Note:  Often for \"classic\" machine learning algorithms the minimum is unique and can be found to high precision. This is not the case for all models:  we will see that Neural Networks often have extremely complex loss landscapes with many local minima and saddles.\n",
    "\n",
    "#### Penalizing large parameters\n",
    "\n",
    "We can modify our loss function to penalize \"large\" parameters. \n",
    "\n",
    "$$\n",
    "\\ell(\\theta) + \\alpha\\operatorname{Size}(\\theta)\n",
    "$$\n",
    "\n",
    "Here $\\alpha$ is an adjustable constant which we will call a <i>hyperparameter</i>: a parameter that is not learned during the training of an estimator.  Different ways of measuring the size of a vector lead to different regularization methods.\n",
    "\n",
    "For $\\alpha=0$ we recover the unregularized estimate for $\\theta$, for $\\alpha=\\infty$ we get $\\theta=0$, values of $\\alpha$ between those two extremes will give different parameter estimates. The value of $\\alpha$ that gives the best model for your data depends on the problem and can be found through *nested* cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba779f77",
   "metadata": {},
   "source": [
    "## Specific regularization models\n",
    "\n",
    "<i>Ridge regression</i> and <i>lasso</i> are two forms of regularization where we make specific choices of how to measure the \"size\" of the parameters.\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "In ridge regression we use the the size of $\\theta$ as the square of the Euclidean length (or \"$\\ell_2$-norm\") of $\\theta$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Size}_{\\textrm{Ridge}}(\\theta) = ||\\theta||^2_2 = \\theta_1^2 + \\theta_2^2 + \\dots + \\theta_p^2.\n",
    "$$\n",
    "\n",
    "This is the length you get using the Pythogorean Theorem!\n",
    "\n",
    "### Lasso regression\n",
    "\n",
    "In lasso regression we take $\\operatorname{Size}(\\theta)$ to be the $\\ell_1$-norm:\n",
    "\n",
    "$$\n",
    "\\operatorname{Size}_{\\textrm{Lasso}}(\\theta)  = ||\\theta||_1 = |\\theta_1| + |\\theta_2| + \\dots + |\\theta_p|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4d221",
   "metadata": {},
   "source": [
    "### Some geometric intuition\n",
    "\n",
    "In the particular case of linear regression, the loss function is quadratic.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"lecture_assets/ridge.png\" width=\"600\" />\n",
    "  <img src=\"lecture_assets/lasso.png\" width=\"618\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abe634",
   "metadata": {},
   "source": [
    "On the left we see a picture of contour lines (in blue) for the MSE of a particular linear regression problem with two features.  The purple circles are the contours of the $\\ell_2$ norm.  The OLS estimate, which minimizes the MSE alone, is at the center of all of the ellipses.  The origin minimizes the $\\ell_2$ norm.  Ridge regression gives us a \"tug of war\" between these two quantities. Note that a ridge regression solution $\\hat{\\theta}_\\alpha$ must be at a point of tangency:  if it were at a point of transverse intersection between contours you could move along one contour while decreasing the other.  We can see some different ridge estimates here:  the small $\\alpha$ are close to the OLS estimates, while large $\\alpha$ is close to the origin.\n",
    "\n",
    "We get a similar picture for Lasso regression, but the contours of the $\\ell_1$ norm are squares instead of circles!  The same argument about tangency applies *until* we intersect a coordinate axis.  Notice that it is clear, from this picture, that the $\\hat{\\theta}_\\alpha$ must follow a **piecewise linear** path as we vary $\\alpha$ (a fact which has some cool applications)!\n",
    "\n",
    "We can also see that while Ridge regression will never zero out a parameter, Lasso will!  In this way, Lasso can be used for \"automatic feature selection\". \n",
    "\n",
    "Let's see this play out by fitting a degree $10$ polynomial our data using both Ridge and Lasso regression and seeing how the coefficients change as we adjust $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3709b",
   "metadata": {},
   "source": [
    "### Implementing in `sklearn`\n",
    "\n",
    "We can implement both of these linear models in `sklearn` with `Ridge` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html</a> for ridge regression and `Lasso` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html</a> for lasso regression.\n",
    "\n",
    "<i>Note: ridge and lasso regression are examples of algorithms/models where scaling the data is a step that should be taken prior to fitting the model. This is because vastly different scales can impact the scales of the components of $\\theta$. This can make it so that there is not enough room in the $\\theta$-budget to afford the actual values of the individual coefficients.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ed3776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the models here\n",
    "## Ridge and Lasso regression are stored in linear_model\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e622ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the ridge pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the ridge regression model\n",
    "    ridge_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('ridge', Ridge(alpha=alpha[i], max_iter=5000000))\n",
    "                              ])\n",
    "    \n",
    "    ## set up the lasso pipeline\n",
    "    ## same steps as with ridge\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the ridge\n",
    "    ridge_pipe.fit(x.reshape(-1,1), y)\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    \n",
    "    # record the coefficients\n",
    "    ridge_coefs[i,:] = ridge_pipe['ridge'].coef_\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc52e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Coefficients\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x^1</th>\n",
       "      <th>x^2</th>\n",
       "      <th>x^3</th>\n",
       "      <th>x^4</th>\n",
       "      <th>x^5</th>\n",
       "      <th>x^6</th>\n",
       "      <th>x^7</th>\n",
       "      <th>x^8</th>\n",
       "      <th>x^9</th>\n",
       "      <th>x^10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha=1e-05</th>\n",
       "      <td>-0.532096</td>\n",
       "      <td>-3.291722</td>\n",
       "      <td>-4.387906</td>\n",
       "      <td>11.909853</td>\n",
       "      <td>5.025352</td>\n",
       "      <td>-9.183685</td>\n",
       "      <td>-2.281569</td>\n",
       "      <td>3.139858</td>\n",
       "      <td>0.345949</td>\n",
       "      <td>-0.391730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.0001</th>\n",
       "      <td>-0.534826</td>\n",
       "      <td>-3.258441</td>\n",
       "      <td>-4.375907</td>\n",
       "      <td>11.821701</td>\n",
       "      <td>5.010790</td>\n",
       "      <td>-9.100585</td>\n",
       "      <td>-2.274959</td>\n",
       "      <td>3.107628</td>\n",
       "      <td>0.344945</td>\n",
       "      <td>-0.387338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.001</th>\n",
       "      <td>-0.561268</td>\n",
       "      <td>-2.953679</td>\n",
       "      <td>-4.259673</td>\n",
       "      <td>11.014577</td>\n",
       "      <td>4.869705</td>\n",
       "      <td>-8.339812</td>\n",
       "      <td>-2.210916</td>\n",
       "      <td>2.812591</td>\n",
       "      <td>0.335212</td>\n",
       "      <td>-0.347132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.01</th>\n",
       "      <td>-0.760828</td>\n",
       "      <td>-1.358686</td>\n",
       "      <td>-3.380449</td>\n",
       "      <td>6.796907</td>\n",
       "      <td>3.801530</td>\n",
       "      <td>-4.369169</td>\n",
       "      <td>-1.725795</td>\n",
       "      <td>1.274186</td>\n",
       "      <td>0.261468</td>\n",
       "      <td>-0.137647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.1</th>\n",
       "      <td>-1.228197</td>\n",
       "      <td>0.381332</td>\n",
       "      <td>-1.262057</td>\n",
       "      <td>2.294823</td>\n",
       "      <td>1.199363</td>\n",
       "      <td>-0.208024</td>\n",
       "      <td>-0.537202</td>\n",
       "      <td>-0.314194</td>\n",
       "      <td>0.080159</td>\n",
       "      <td>0.076043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=1</th>\n",
       "      <td>-1.273637</td>\n",
       "      <td>0.897989</td>\n",
       "      <td>-0.530516</td>\n",
       "      <td>1.197639</td>\n",
       "      <td>0.093808</td>\n",
       "      <td>0.472310</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>-0.450955</td>\n",
       "      <td>-0.006424</td>\n",
       "      <td>0.079763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=10</th>\n",
       "      <td>-0.887189</td>\n",
       "      <td>0.696417</td>\n",
       "      <td>-0.477340</td>\n",
       "      <td>0.713626</td>\n",
       "      <td>-0.171016</td>\n",
       "      <td>0.411016</td>\n",
       "      <td>0.057622</td>\n",
       "      <td>-0.106703</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>-0.011332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=100</th>\n",
       "      <td>-0.319023</td>\n",
       "      <td>0.221442</td>\n",
       "      <td>-0.257106</td>\n",
       "      <td>0.286443</td>\n",
       "      <td>-0.223319</td>\n",
       "      <td>0.306501</td>\n",
       "      <td>-0.154086</td>\n",
       "      <td>0.229684</td>\n",
       "      <td>0.063839</td>\n",
       "      <td>-0.092531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=1000</th>\n",
       "      <td>-0.070440</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>-0.077307</td>\n",
       "      <td>0.093393</td>\n",
       "      <td>-0.091999</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>-0.091101</td>\n",
       "      <td>0.139988</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>-0.026169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x^1       x^2       x^3        x^4       x^5       x^6  \\\n",
       "alpha=1e-05  -0.532096 -3.291722 -4.387906  11.909853  5.025352 -9.183685   \n",
       "alpha=0.0001 -0.534826 -3.258441 -4.375907  11.821701  5.010790 -9.100585   \n",
       "alpha=0.001  -0.561268 -2.953679 -4.259673  11.014577  4.869705 -8.339812   \n",
       "alpha=0.01   -0.760828 -1.358686 -3.380449   6.796907  3.801530 -4.369169   \n",
       "alpha=0.1    -1.228197  0.381332 -1.262057   2.294823  1.199363 -0.208024   \n",
       "alpha=1      -1.273637  0.897989 -0.530516   1.197639  0.093808  0.472310   \n",
       "alpha=10     -0.887189  0.696417 -0.477340   0.713626 -0.171016  0.411016   \n",
       "alpha=100    -0.319023  0.221442 -0.257106   0.286443 -0.223319  0.306501   \n",
       "alpha=1000   -0.070440  0.057039 -0.077307   0.093393 -0.091999  0.131640   \n",
       "\n",
       "                   x^7       x^8       x^9      x^10  \n",
       "alpha=1e-05  -2.281569  3.139858  0.345949 -0.391730  \n",
       "alpha=0.0001 -2.274959  3.107628  0.344945 -0.387338  \n",
       "alpha=0.001  -2.210916  2.812591  0.335212 -0.347132  \n",
       "alpha=0.01   -1.725795  1.274186  0.261468 -0.137647  \n",
       "alpha=0.1    -0.537202 -0.314194  0.080159  0.076043  \n",
       "alpha=1       0.009133 -0.450955 -0.006424  0.079763  \n",
       "alpha=10      0.057622 -0.106703  0.001682 -0.011332  \n",
       "alpha=100    -0.154086  0.229684  0.063839 -0.092531  \n",
       "alpha=1000   -0.091101  0.139988  0.006748 -0.026169  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ridge Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(ridge_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc4fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x^1</th>\n",
       "      <th>x^2</th>\n",
       "      <th>x^3</th>\n",
       "      <th>x^4</th>\n",
       "      <th>x^5</th>\n",
       "      <th>x^6</th>\n",
       "      <th>x^7</th>\n",
       "      <th>x^8</th>\n",
       "      <th>x^9</th>\n",
       "      <th>x^10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha=1e-05</th>\n",
       "      <td>-0.538353</td>\n",
       "      <td>-3.250257</td>\n",
       "      <td>-4.359495</td>\n",
       "      <td>11.800042</td>\n",
       "      <td>4.990309</td>\n",
       "      <td>-9.080023</td>\n",
       "      <td>-2.265502</td>\n",
       "      <td>3.099587</td>\n",
       "      <td>0.343490</td>\n",
       "      <td>-0.386233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.0001</th>\n",
       "      <td>-0.597412</td>\n",
       "      <td>-2.843806</td>\n",
       "      <td>-4.091758</td>\n",
       "      <td>10.723675</td>\n",
       "      <td>4.660312</td>\n",
       "      <td>-8.064071</td>\n",
       "      <td>-2.114270</td>\n",
       "      <td>2.704956</td>\n",
       "      <td>0.320356</td>\n",
       "      <td>-0.332374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.001</th>\n",
       "      <td>-1.201256</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.353878</td>\n",
       "      <td>3.251137</td>\n",
       "      <td>1.285138</td>\n",
       "      <td>-1.059765</td>\n",
       "      <td>-0.567253</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.083677</td>\n",
       "      <td>0.035022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.01</th>\n",
       "      <td>-1.434115</td>\n",
       "      <td>0.570207</td>\n",
       "      <td>-0.264264</td>\n",
       "      <td>1.808744</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>-0.284984</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.057999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=0.1</th>\n",
       "      <td>-1.277036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.081929</td>\n",
       "      <td>1.501704</td>\n",
       "      <td>-0.137108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>-0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=1</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.167810</td>\n",
       "      <td>0.475141</td>\n",
       "      <td>-0.059550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=10</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036141</td>\n",
       "      <td>0.042068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=100</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.003079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha=1000</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x^1       x^2       x^3        x^4       x^5       x^6  \\\n",
       "alpha=1e-05  -0.538353 -3.250257 -4.359495  11.800042  4.990309 -9.080023   \n",
       "alpha=0.0001 -0.597412 -2.843806 -4.091758  10.723675  4.660312 -8.064071   \n",
       "alpha=0.001  -1.201256 -0.000000 -1.353878   3.251137  1.285138 -1.059765   \n",
       "alpha=0.01   -1.434115  0.570207 -0.264264   1.808744 -0.000000 -0.000000   \n",
       "alpha=0.1    -1.277036  0.000000 -0.081929   1.501704 -0.137108  0.000000   \n",
       "alpha=1      -0.000000  0.000000 -0.000000   0.000000 -0.167810  0.475141   \n",
       "alpha=10     -0.000000  0.000000 -0.000000   0.000000 -0.000000  0.000000   \n",
       "alpha=100    -0.000000  0.000000 -0.000000   0.000000 -0.000000  0.000000   \n",
       "alpha=1000   -0.000000  0.000000 -0.000000   0.000000 -0.000000  0.000000   \n",
       "\n",
       "                   x^7       x^8       x^9      x^10  \n",
       "alpha=1e-05  -2.265502  3.099587  0.343490 -0.386233  \n",
       "alpha=0.0001 -2.114270  2.704956  0.320356 -0.332374  \n",
       "alpha=0.001  -0.567253 -0.000000  0.083677  0.035022  \n",
       "alpha=0.01   -0.002152 -0.284984 -0.000000  0.057999  \n",
       "alpha=0.1    -0.000000 -0.000000  0.005172 -0.022312  \n",
       "alpha=1      -0.059550  0.000000  0.000000 -0.014803  \n",
       "alpha=10     -0.000000  0.000000 -0.036141  0.042068  \n",
       "alpha=100    -0.000000  0.000000 -0.000000  0.003079  \n",
       "alpha=1000   -0.000000  0.000000 -0.000000  0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Lasso Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(lasso_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dda9c",
   "metadata": {},
   "source": [
    "### An important note about scaling.\n",
    "\n",
    "OLS linear regression is scale invariant.  That means that if you scale your features and run OLS linear regression you will get exactly the same predictions as if you had not scaled them.  Another way of thinking about this is that OLS will give you the same results no matter what units you use for your features:  for example if $x_1$ is height in meters and $x_1'$ is height in centimeters, then the OLS estimates $\\hat{\\theta}_1$ and $\\hat{\\theta}_1'$ would be related by $\\hat{\\theta}_1' = 100 \\hat{\\theta}_1$.  Changing the unit from m to cm just changes the unit of $\\theta$ from $\\frac{\\textrm{units}}{m}$ to $\\frac{\\textrm{units}}{cm}$.\n",
    "\n",
    "Both Ridge and Lasso regression **are not** scale invariant.  It is easy to see why: if we change from $m$ to $cm$ the ``size'' the OLS parameters will change by a factor of $\\frac{1}{100}$.  As a consequence, both Ridge and Lasso will prioritize keeping these predictors in the model, since the coefficient is not very \"expensive\" in terms of parameter size but does a lot to decrease the MSE.\n",
    "\n",
    "To avoid this it is highly advisable to *scale and center your data* before Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c65ea",
   "metadata": {},
   "source": [
    "## Ridge and Lasso Regularization in other sklearn models\n",
    "\n",
    "## Models with Explicit L2 (Ridge-style) Regularization\n",
    "- **Ridge** — parameter: `alpha`\n",
    "- **LogisticRegression** (`penalty=\"l2\"`) — parameter: `C` (inverse strength)\n",
    "- **LinearSVC**, **LinearSVR** (`penalty=\"l2\"`) — parameter: `C`\n",
    "- **MLPClassifier / MLPRegressor** — parameter: `alpha`\n",
    "- **ElasticNet** (`l1_ratio < 1`) — parameters: `alpha`, `l1_ratio`\n",
    "- **SGDClassifier / SGDRegressor** (`penalty=\"l2\"`) — parameter: `alpha`\n",
    "\n",
    "## Models with Explicit L1 (Lasso-style) Regularization\n",
    "- **Lasso** — parameter: `alpha`\n",
    "- **LogisticRegression** (`penalty=\"l1\"`, solver=`'saga'` or `'liblinear'`) — parameter: `C`\n",
    "- **LinearSVC** (`penalty=\"l1\"`, solver=`'liblinear'`) — parameter: `C`\n",
    "- **ElasticNet** (`l1_ratio > 0`) — parameters: `alpha`, `l1_ratio`\n",
    "- **SGDClassifier / SGDRegressor** (`penalty=\"l1\"`) — parameter: `alpha`\n",
    "\n",
    "## Models with ElasticNet (L1 + L2) Regularization\n",
    "- **ElasticNet** (`sklearn.linear_model.ElasticNet`) — parameters: `alpha`, `l1_ratio`\n",
    "- **LogisticRegression** (`penalty=\"elasticnet\"`, solver=`'saga'`) — parameters: `C`, `l1_ratio`\n",
    "- **SGDClassifier / SGDRegressor** (`penalty=\"elasticnet\"`) — parameters: `alpha`, `l1_ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ec113",
   "metadata": {},
   "source": [
    "## Which one to use?\n",
    "\n",
    "Which type of regularization is the better choice? Well that depends on the problem. Both are good at addressing overfitting concerns, but each has a couple unique pros and cons.\n",
    "\n",
    "##### Lasso\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "- Works well when you have a large number of features that do not have any effect on the target\n",
    "- Feature selection is a plus, this can allow for a sparser model which is good for computational reasons.\n",
    "- Feature selection can also produce a more interpretable model.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "- Can have trouble with highly correlated features (colinearity), it typically chooses one variable among those that are correlated, which may be random.\n",
    "\n",
    "##### Ridge\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "- Works well when the target depends on all or most of the features and\n",
    "- Can handle colinearity better than lasso.\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "- Because ridge typically keeps most of the predictors in the model, this can be a computationally costly model type for data sets with a large number of predictors.\n",
    "- Keeping all features also makes interpretation of the model difficult.\n",
    "\n",
    "##### Elastic Net\n",
    "\n",
    "Sometimes the best model will have both types of regularization penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020ee60",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
