{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dfc94b",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "A *hyperparameter* is a parameter that is not learned during the training of an estimator.  *Tuning* a hyperparameter is the process of searching for hyperparameter values which optimize the performance of the estimator.  The primary danger we need to guard against is over-fitting, and our primary defense will be *nested cross-validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9eb0c",
   "metadata": {},
   "source": [
    "We have already seen the regularization hyperparameters of the Ridge and Lasso linear models, as well as the number of components in Principle Component Analysis.  \n",
    "\n",
    "The documentation for any estimator should include information about its hyperparameters. You can use the `.get_params()` method of an instantiated estimator returns a dictionary of its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0619619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': None,\n",
       " 'max_features': 1.0,\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# The .get_params() method of an instantiated estimator returns a dictionary of its hyperparameters.\n",
    "RandomForestRegressor().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99427824",
   "metadata": {},
   "source": [
    "## Nested Cross-validation\n",
    "\n",
    "Nested cross-validation works in two layers:\n",
    "\n",
    "- Outer loop (evaluation): The data is split into train/test folds, just like in standard cross-validation. Each outer test fold is held out to estimate performance.\n",
    "\n",
    "- Inner loop (tuning): Within each outer training set, another cross-validation is run to select the best hyperparameters.\n",
    "\n",
    "For each outer split, hyperparameters are tuned only on the training portion (inner CV), and the resulting model is then tested on the outer holdout. Averaging across all outer folds provides an unbiased estimate of how the tuned model will perform on unseen data.\n",
    "\n",
    "Without nested cross-validation, there’s a risk of overfitting to a particular split: among the many hyperparameter combinations tried, some may appear to perform well on that split purely by chance, but won’t generalize as well to new data.\n",
    "\n",
    "We demonstrate that now by fitting `RandomForestClassifier` to data where the target values are draws from a Bernoulli random variable and are completely independent from the target.  We will see that using regular cross-validation to select the best hyperparameters results in overfitting, while nested cross-validation does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc05aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single CV tuning (likely overfit):\n",
      "Best hyperparameters: {'bootstrap': True, 'max_depth': 34, 'max_features': np.float64(0.13017919126220145), 'min_samples_leaf': 6, 'min_samples_split': 3, 'n_estimators': 59}\n",
      "Validation Accuracy: 0.6428571428571429\n",
      "Test Accuracy: 0.43333333333333335\n",
      "\n",
      "Nested CV (unbiased estimate):\n",
      "Inner fold accuracies: [0.6001899335232669, 0.600664767331434, 0.6229819563152897, 0.6372269705603039, 0.7378917378917379]\n",
      "Outer fold accuracies: [0.5, 0.65, 0.45, 0.65, 0.5]\n",
      "Mean Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "rng = np.random.default_rng(231234)\n",
    "\n",
    "# Generate random features and random binary labels (pure noise)\n",
    "X = rng.normal(size=(100, 10))  # 200 samples, 10 features\n",
    "y = rng.binomial(1, 0.5, size=100)  # labels 0 or 1, 50/50\n",
    "\n",
    "# Single train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Hyperparameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    \"n_estimators\": randint(50, 500),\n",
    "    \"max_depth\": randint(2, 50),\n",
    "    \"max_features\": uniform(0.1, 0.9),\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 10),\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# 1) Hyperparameter tuning with single CV (overfitting example)\n",
    "search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions,\n",
    "    n_iter=200,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "single_cv_val = search.best_score_\n",
    "single_cv_test = accuracy_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "print(\"Single CV tuning (likely overfit):\")\n",
    "print(\"Best hyperparameters:\", search.best_params_)\n",
    "print(\"Validation Accuracy:\", single_cv_val)\n",
    "print(\"Test Accuracy:\", single_cv_test)\n",
    "\n",
    "\n",
    "# 2) Nested CV\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nested_scores = []\n",
    "inner_cv_scores = []\n",
    "\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X):\n",
    "    X_outer_train, X_outer_test = X[train_idx], X[test_idx]\n",
    "    y_outer_train, y_outer_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Inner CV for hyperparameter tuning\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    inner_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions,\n",
    "        n_iter=200,\n",
    "        cv=inner_cv,\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    inner_search.fit(X_outer_train, y_outer_train)\n",
    "\n",
    "    inner_cv_scores.append(float(inner_search.best_score_))\n",
    "\n",
    "    \n",
    "    # Evaluate best model on outer test fold\n",
    "    best_inner_model = clone(inner_search.best_estimator_)\n",
    "    best_inner_model.fit(X_outer_train, y_outer_train)\n",
    "    acc = accuracy_score(y_outer_test, best_inner_model.predict(X_outer_test))\n",
    "    nested_scores.append(acc)\n",
    "\n",
    "print(\"\\nNested CV (unbiased estimate):\")\n",
    "print(\"Inner fold accuracies:\", inner_cv_scores)\n",
    "print(\"Outer fold accuracies:\", nested_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(nested_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
